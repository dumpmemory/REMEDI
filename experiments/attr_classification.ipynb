{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3668413b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbd381a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import model_utils\n",
    "\n",
    "import transformers\n",
    "\n",
    "config = \"gpt2-xl\"\n",
    "device = \"cuda:15\"\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config)\n",
    "model.to(device).eval()\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "mt = model_utils.ModelAndTokenizer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6b1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import dataset_utils\n",
    "\n",
    "dataset = dataset_utils.load_dataset(\"counterfact\", split=\"train[:10000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f41f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mt.model.eval()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def is_known_by_model(batch):\n",
    "    prompts = batch[\"prompt\"]\n",
    "    targets = batch[\"target_unmediated\"]\n",
    "    inputs = mt.tokenizer(\n",
    "        prompts,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"longest\",\n",
    "        truncation=True,\n",
    "    ).to(device)\n",
    "    outputs = mt.model(**inputs)\n",
    "    batch_idx = torch.arange(len(prompts))\n",
    "    token_idx = inputs.attention_mask.sum(dim=-1) - 1\n",
    "    predictions = outputs.logits[batch_idx, token_idx].topk(dim=-1, k=5).indices\n",
    "    \n",
    "    batched_tokens = [\n",
    "        [\n",
    "            token.replace(\"Ä \", \" \").strip().lower()\n",
    "            for token in tokenizer.convert_ids_to_tokens(prediction)\n",
    "        ]\n",
    "        for prediction in predictions\n",
    "    ]\n",
    "\n",
    "    return [\n",
    "        target.lower() in tokens\n",
    "        for target, tokens in zip(targets, batched_tokens)\n",
    "    ]\n",
    "\n",
    "dataset = dataset.filter(is_known_by_model, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "import torch\n",
    "\n",
    "LAYER = 30\n",
    "\n",
    "@torch.inference_mode()\n",
    "def precompute_hiddens(batch):\n",
    "    entities = batch[\"entity\"]\n",
    "    prompts = batch[\"prompt\"]\n",
    "\n",
    "    targets_mediated = batch[\"target_mediated\"]\n",
    "    targets_unmediated = batch[\"target_unmediated\"]\n",
    "\n",
    "    attributes_mediated = batch[\"context\"]\n",
    "    attributes_unmediated = [\n",
    "        attribute.replace(tm, tum)\n",
    "        for attribute, tm, tum in zip(\n",
    "            attributes_mediated,\n",
    "            targets_mediated,\n",
    "            targets_unmediated,\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    outputs = {}\n",
    "    batch_idx = torch.arange(len(entities))\n",
    "    for key, text in (\n",
    "        (\"entity\", entities),\n",
    "        (\"attribute_mediated\", attributes_mediated),\n",
    "        (\"attribute_unmediated\", attributes_unmediated),\n",
    "    ):\n",
    "        inputs = mt.tokenizer(text,\n",
    "                              return_tensors=\"pt\",\n",
    "                              padding=\"longest\",\n",
    "                              truncation=True).to(device)\n",
    "        token_idx = inputs.attention_mask.sum(dim=-1) - 1\n",
    "        with baukit.Trace(mt.model, f\"transformer.h.{LAYER}\", stop=True) as ret:\n",
    "            mt.model(**inputs)\n",
    "        hiddens = ret.output[0][batch_idx, token_idx]\n",
    "\n",
    "#         counts = inputs.attention_mask.sum(dim=-1, keepdim=True)\n",
    "#         counts[counts == 0] = 1\n",
    "#         hiddens = ret.output[0]\\\n",
    "#             .mul(inputs.attention_mask[..., None])\\\n",
    "#             .sum(dim=1)\\\n",
    "#             .div(counts)\n",
    "\n",
    "        outputs[f\"{key}.rep\"] = hiddens\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d5ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "precomputed = dataset.map(precompute_hiddens, batched=True, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a773b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import dataset_utils, training_utils\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "LR = 1e-3\n",
    "MAX_EPOCHS = 100\n",
    "BATCH_SIZE = 256\n",
    "PATIENCE = 10\n",
    "HOLD_OUT = .1\n",
    "\n",
    "hidden_size = mt.model.config.hidden_size\n",
    "probe = nn.Bilinear(hidden_size, hidden_size, 1)\n",
    "probe.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(probe.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "stopper = training_utils.EarlyStopping(patience=PATIENCE, decreasing=False)\n",
    "\n",
    "def make_inputs(batch):\n",
    "    entity_reps = batch[\"entity.rep\"].to(device)\n",
    "    attr_unmed_reps = batch[\"attribute_unmediated.rep\"].to(device)\n",
    "    attr_med_reps = batch[\"attribute_mediated.rep\"].to(device)\n",
    "    batch_size = len(entity_reps)\n",
    "\n",
    "    entity_reps = torch.cat([entity_reps, entity_reps])\n",
    "    attr_reps = torch.cat([\n",
    "        attr_unmed_reps,\n",
    "        attr_med_reps,\n",
    "    ])\n",
    "\n",
    "    labels = torch.empty(2 * batch_size, device=device)\n",
    "    labels[:batch_size] = 1\n",
    "    labels[batch_size:] = 0\n",
    "    return entity_reps, attr_reps, labels\n",
    "    \n",
    "\n",
    "precomputed = dataset_utils.maybe_train_test_split(precomputed, test_size=HOLD_OUT)\n",
    "with precomputed.formatted_as(\"torch\"):\n",
    "    train_loader = DataLoader(precomputed[\"train\"], batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(precomputed[\"test\"], batch_size=BATCH_SIZE)\n",
    "    best = probe.state_dict()\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        probe.train()\n",
    "        train_loss = 0.\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            entity_reps, attr_reps, labels = make_inputs(batch)\n",
    "            logits = probe(entity_reps, attr_reps).squeeze()\n",
    "            loss = criterion(logits, labels)\n",
    "            if epoch != 0:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        probe.eval()\n",
    "        correct, total = 0, 0\n",
    "        for batch in val_loader:\n",
    "            entity_reps, attr_reps, labels = make_inputs(batch)\n",
    "            batch_size = len(entity_reps)\n",
    "            with torch.inference_mode():\n",
    "                logits = probe(entity_reps, attr_reps).view(batch_size)\n",
    "            predictions = torch.sigmoid(logits).gt(.5)\n",
    "            correct += predictions.eq(labels.bool()).sum()\n",
    "            total += batch_size\n",
    "        val_accuracy = correct / total\n",
    "\n",
    "        print(f\"epoch {epoch} / train {train_loss:.2f} / val {val_accuracy:.4f}\")\n",
    "        if stopper(val_accuracy):\n",
    "            print(\"patience reached, stopping\")\n",
    "            model.load_state_dict(best)\n",
    "            break\n",
    "        elif stopper.improved:\n",
    "            best = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97219814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate(entity, attribute):\n",
    "    attribute = attribute.format(entity)\n",
    "    args = []\n",
    "    for text in (entity, attribute):\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "        with baukit.Trace(mt.model, f\"transformer.h.{LAYER}\") as ret:\n",
    "            mt.model(**inputs)\n",
    "        hidden = ret.output[0][:, -1]\n",
    "        args.append(hidden)\n",
    "    logit = model(*args)\n",
    "    return torch.sigmoid(logit).gt(.5).item()\n",
    "\n",
    "evaluate(\"Barack Obama\", \"{} is from Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ac757b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
