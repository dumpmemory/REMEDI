{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36487546",
   "metadata": {},
   "source": [
    "# Attribute Lens Experiment for GPT-2-XL\n",
    "\n",
    "This notebook contains the \"attribute lens\" initial experiment based on this idea from discord.\n",
    "\n",
    "Say `D(h)` is the decoder readout head.  Logit lens had the idea of visualizing every hidden state `h[layer,token]` in a transformer looking at `D(h[layer,token])`.\n",
    "\n",
    "Well instead of using D(h), why don't we look at hidden states using F(h) where F is a longer computation through the transformer rather than just the decoder head?  This is directly connected to our causal traces:\n",
    "\n",
    "We noticed that (surprisingly!) single-hidden states early in the network can be causal for a much later token prediction.    So for example if you have a generic sentence like \"*** is located in the city of\", where the subject has been noised out, there is a specific early causal layer and token state `h[Lc,Tc]` where you can jam in a particular vector and it will say \"Seattle\" or \"Paris\" or \"Rome\" many tokens later.....\n",
    "\n",
    "So let's fix the corrupted sentence form, and fix the causal layer and token `Lc` and `Tc` at the early site where the individual hidden state is causal for the object-attribute word.  And let's define `F(h)` as the decoding of what we get if you set `h[Lc, Tc] := h`   and run the computation in this sentence form.  At the end of the corrupted-and-intervened form, GPT will make a prediction, and this prediction is the output of `F(h)`.\n",
    "\n",
    "\n",
    "OK, we begin by loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47ad23",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67160ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, baukit\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#MODEL_NAME = \"gpt2-xl\"  # gpt2-xl or EleutherAI/gpt-j-6B\n",
    "MODEL_NAME = \"EleutherAI/gpt-j-6B\"\n",
    "model, tok = (\n",
    "    AutoModelForCausalLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=False).to(device),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    ")\n",
    "baukit.set_requires_grad(False, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04240bfa",
   "metadata": {},
   "source": [
    "So the innovation here is to make a custom readout function that is different from the standard readout.\n",
    "\n",
    "\n",
    "The `make_custom_readout` function below is supposed to do that (but it seems to have some bugs).\n",
    "\n",
    "Basically it does this:\n",
    "  * As input it takes the custom readout `prompt` such as \". * is located in the city of\", where the user has provided a star in the location of the \"unknown\" subject.\n",
    "  * And it also takes a `layer` at which we're going to start driving the hidden state to test.\n",
    "  * Then it creates a function `custom_readout(h)` that does the following:\n",
    "  \n",
    "`custom readout` does this:\n",
    "  * if you pass it many hidden vectors `h`, it flattens the tensor into a big batch of `input_size` via vector-dimensions.\n",
    "  * For each `h` (input_size times) it does this:\n",
    "    * Runs the `prompt` 10 times\n",
    "    * each time subsituting random noise in embeddings for the words at the `*` symbols.\n",
    "    * each time it will also insert the vector `h` into the last `*` token at the given `layer`.\n",
    "    * Then it reads out the prediction logits of the 10 times, and stores the logits\n",
    "  * Finally after everything is done, it converts the logits into probabilities using softmax, and averages batches of 10 runs to get the results.\n",
    "  \n",
    "Then `make_custom_readout` returns this customized  `custom_readout` function, which maps `h -> token predictions` so that it can be used as a logit lens decoder.\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eb062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_embedding_scale(model, tok):\n",
    "    prompt = 'Jane John Elizabeth Gerald Washington Seattle London Paris Boston'\n",
    "    inp = {k: torch.tensor(v)[None].to(device) for k, v in tok(prompt).items()}\n",
    "    embed_layer = [n for n, _ in model.named_modules() if 'wte' in n or 'embed' in n][0]\n",
    "    with baukit.Trace(model, embed_layer) as t:\n",
    "        model(**inp)\n",
    "        return t.output.std()\n",
    "\n",
    "# This function just returns random noise\n",
    "noise_cache = None\n",
    "def fixed_noise(shape):\n",
    "    import numpy\n",
    "    global noise_cache\n",
    "    amount = numpy.prod(shape)\n",
    "    if noise_cache is None or len(noise_cache) < amount:\n",
    "        noise_cache = torch.Tensor(numpy.random.RandomState(1).randn(amount)).to(device)\n",
    "    return noise_cache[:amount].reshape(shape)\n",
    "\n",
    "def make_custom_readout(model, tok, prompt, subject='*', layer=None, noise_samples=10):\n",
    "    # Layer should be a layer name\n",
    "    target_layer = layer\n",
    "\n",
    "    # These are all the token used in the subject name.  We encode it twice to get\n",
    "    # both the non-space-prefixed and space-prefixed ones if needed.\n",
    "    star_tokens = tok.encode(f'{subject} {subject}')\n",
    "\n",
    "    # Tokenize the prompt and put it on cuda\n",
    "    inp = {k: torch.tensor(v)[None].to(device) for k, v in tok(prompt).items()}\n",
    "\n",
    "    # Find the index locations where the star/subject tokens appear.\n",
    "    # This line used to have a bug.\n",
    "    star_indexes = sum(inp['input_ids'] == t for t in star_tokens)[0].nonzero()[:,0].tolist()\n",
    "    index = star_indexes[-1]\n",
    "\n",
    "    # Get the embedding layer name\n",
    "    embed_layers = [n for n, _ in model.named_modules() if 'wte' in n or 'embed' in n]\n",
    "    noise_level = get_embedding_scale(model, tok)\n",
    "    gt = lambda x: noise_level * x\n",
    "    # The following rule was studied in the ROME rebuttal but doesn't seem to help for us.\n",
    "    # from lens.stats import collect_embedding_gaussian\n",
    "    # gt = collect_embedding_gaussian(model, tok).to('cuda', torch.float)\n",
    "    \n",
    "    if layer is None:\n",
    "        layers = [n for n, _ in model.named_modules() if re.match(r'^transformer.h.\\d+$', n)]\n",
    "    else:\n",
    "        layers = [layer]\n",
    "\n",
    "    def custom_readout(h):\n",
    "        import numpy\n",
    "        cuda_h = h.to(device)\n",
    "        # h may be a tensor of state vectors.  Flatten it to a batch.\n",
    "        input_size = int(numpy.prod(cuda_h.shape[:-1]))\n",
    "        flat_h = cuda_h.reshape(input_size, cuda_h.shape[-1])\n",
    "        # We will physically batch by noise samples, and then just loop over input batch for now\n",
    "        batch_size = noise_samples\n",
    "        batch_inp = {k: v.expand((batch_size,) + v.shape[1:]) for k, v in inp.items()}\n",
    "        \n",
    "        the_noise = None\n",
    "        results = []\n",
    "        for input_index in range(0, input_size):\n",
    "            target_h = flat_h[input_index][None]\n",
    "            def insert_state(x, layer):\n",
    "                hs = x[0] if isinstance(x, tuple) else x\n",
    "                if layer in embed_layers:\n",
    "                    hs[:,star_indexes,:] = gt(fixed_noise(hs[:,star_indexes,:].shape))\n",
    "                else: # layer == target_layer:\n",
    "                    hs[:,index,:] = target_h\n",
    "                return x\n",
    "            with baukit.TraceDict(model, layers + embed_layers, edit_output=insert_state) as t:\n",
    "                batch_results = model(**batch_inp)['logits'][:,-1,:]\n",
    "            results.append(batch_results)\n",
    "        raw_results = torch.stack(results).reshape(\n",
    "            *(cuda_h.shape[:-1] + (noise_samples,) + batch_results.shape[-1:]))\n",
    "        return torch.nn.functional.softmax(raw_results, dim=-1).mean(dim=-2)\n",
    "    return custom_readout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b15ee86",
   "metadata": {},
   "source": [
    "Now we can test things.  If we pass a tensor of state vectors, we should get a tensor of predictions out instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24361a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = make_custom_readout(model, tok, 'The * is located in the city of')\n",
    "\n",
    "\n",
    "# Try running f on some zero vectors\n",
    "probs = f(torch.zeros(1, 5, 1, 3, 2, 4096))\n",
    "print(probs.shape)\n",
    "probs.sum(dim=-1).flatten()  # Veriy that Probabilities add up to 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab007e8",
   "metadata": {},
   "source": [
    "This function gathers the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_states(model, tok, prefix):\n",
    "    import re\n",
    "    from baukit import TraceDict\n",
    "    inp = {k: torch.tensor(v)[None].to(device) for k, v in tok(prefix).items()}\n",
    "    layer_names = [n for n, _ in model.named_modules()\n",
    "                   if re.match(r'^transformer.h.\\d+$', n)]\n",
    "    with TraceDict(model, layer_names) as tr:\n",
    "        logits = model(**inp)['logits']\n",
    "    return torch.stack([tr[layername].output[0] for layername in layer_names])\n",
    "\n",
    "prompt = 'Hello, my name is also'\n",
    "hs = get_hidden_states(model, tok, prompt)\n",
    "hs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c4aa29",
   "metadata": {},
   "source": [
    "Here is the basic logit lens visualization.  Comments inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aff42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_logit_lens(model, tok, prefix, topk=5, color=None, hs=None, decoder=None):\n",
    "    from baukit import show\n",
    "\n",
    "    # You can pass in a function to compute the hidden states, or just the tensor of hidden states.\n",
    "    if hs is None:\n",
    "        hs = get_hidden_states\n",
    "    if callable(hs):\n",
    "        hs = hs(model, tok, prefix)\n",
    "\n",
    "    # The full decoder head normalizes hidden state and applies softmax at the end.\n",
    "    if decoder is None:\n",
    "        decoder = torch.nn.Sequential(model.transformer.ln_f, model.lm_head, torch.nn.Softmax(dim=-1))\n",
    "\n",
    "    probs = decoder(hs) # Apply the decoder head to every hidden state\n",
    "    favorite_probs, favorite_tokens = probs.topk(k=topk, dim=-1)\n",
    "    # Let's also plot hidden state magnitudes\n",
    "    magnitudes = hs.norm(dim=-1)\n",
    "    # For some reason the 0th token always has huge magnitudes, so normalize based on subsequent token max.\n",
    "    # Added if statement to handle one token input\n",
    "    if (len(magnitudes[0][0]) > 1):\n",
    "        magnitudes = magnitudes / magnitudes[:,:,1:].max()\n",
    "    \n",
    "    # All the input tokens.\n",
    "    prompt_tokens = [tok.decode(t) for t in tok.encode(prefix)]\n",
    "\n",
    "    # Foreground color shows token probability, and background color shows hs magnitude\n",
    "    if color is None:\n",
    "        color = [0, 0, 255]\n",
    "    def color_fn(m, p):\n",
    "        a = [int(255 * (1-m) + c * m) for c in color]\n",
    "        b = [int(196 * (1-p) + 0 * p)] * 2 + [0]\n",
    "        return show.style(background=f'rgb({a[0]}, {a[1]}, {a[2]})',\n",
    "                          color=f'rgb({b[0]}, {b[1]}, {b[2]})' )\n",
    "\n",
    "    # In the hover popup, show topk probabilities beyond the 0th.\n",
    "    def hover(tok, prob, toks, m):\n",
    "        lines = [f'mag: {m:.2f}']\n",
    "        for p, t in zip(prob, toks):\n",
    "            lines.append(f'{tok.decode(t)}: prob {p:.2f}')\n",
    "        return show.attr(title='\\n'.join(lines))\n",
    "    \n",
    "    # Construct the HTML output using show.\n",
    "    header_line = [ # header line\n",
    "             [[show.style(fontWeight='bold'), 'Layer']] +\n",
    "             [\n",
    "                 [show.style(background='yellow'), show.attr(title=f'Token {i}'), t]\n",
    "                 for i, t in enumerate(prompt_tokens)\n",
    "             ]\n",
    "         ]\n",
    "    layer_logits = [\n",
    "             # first column\n",
    "             [[show.style(fontWeight='bold'), layer]] +\n",
    "             [\n",
    "                 # subsequent columns\n",
    "                 [color_fn(m, p[0]), hover(tok, p, t, m), show.style(overflowX='hide'), tok.decode(t[0])]\n",
    "                 for m, p, t in zip(wordmags, wordprobs, words)\n",
    "             ]\n",
    "        for layer, wordmags, wordprobs, words in\n",
    "                zip(range(len(magnitudes)), magnitudes[:, 0], favorite_probs[:, 0], favorite_tokens[:,0])]\n",
    "    \n",
    "    # If you want to get the html without showing it, use show.html(...)\n",
    "    show(header_line + layer_logits + header_line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae7d715",
   "metadata": {},
   "source": [
    "An example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_logit_lens(model, tok, 'The exhibits at the National Air and Space Museum were', decoder=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52db7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = make_custom_readout(model, tok, '. Arthur Rubinstein plays the piano. Miles Davis plays the trumpet. Yo-Yo Ma plays the cello. * plays the', layer='transformer.h.17')\n",
    "#f = make_custom_readout(model, tok, '* plays the', layer='transformer.h.17')\n",
    "f = make_custom_readout(model, tok, '. * is famous in the field of')\n",
    "\n",
    "\n",
    "show_logit_lens(model, tok, 'Professors Zoubin Ghahramani and Michael Jordan', decoder=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10c18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tok, prefix, n=10):\n",
    "    inp = {k: torch.tensor(v)[None].to(device) for k, v in tok(prefix).items()}\n",
    "    initial_length = len(inp['input_ids'][0])\n",
    "    pkv = None\n",
    "    for _ in range(n):\n",
    "        full_out = model(**inp)\n",
    "        out = full_out['logits']\n",
    "        pred = out[0, -1].argmax()\n",
    "        inp['input_ids'] = torch.cat((inp['input_ids'], torch.tensor([pred])[None].to(device)), dim=1)\n",
    "        inp['attention_mask'] = torch.cat((inp['attention_mask'], torch.ones(1, 1).to(device)), dim=1)\n",
    "    return tok.decode(inp['input_ids'][0, initial_length:])\n",
    "#generate(model, tok, '. Arthur Rubinstein plays the piano. Miles Davis plays the trumpet. Yo-Yo Ma plays the cello. Itzhak Perlman plays the', n=3)\n",
    "\n",
    "generate(model, tok, 'Yo-yo Ma is a famous cellist. Arthur Rubenstein is a famous', n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a4b971",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
