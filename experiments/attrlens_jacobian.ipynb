{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f0468b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "device = \"cuda:5\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"gpt2-xl\")\n",
    "model.eval().to(device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6b1f7c",
   "metadata": {},
   "source": [
    "# Slightly Nicer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde0672b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "lm_head = nn.Sequential(\n",
    "    model.transformer.ln_f,\n",
    "    model.lm_head,\n",
    ")\n",
    "\n",
    "def logit_lens(prompt, entities, layers=range(40, 48), model=model, tokenizer=tokenizer):\n",
    "    batch_idx = torch.arange(len(entities))\n",
    "    prompts = [\n",
    "        prompt.format(entity)\n",
    "        for entity in entities\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    prompt_last_idx = inputs.attention_mask.sum(dim=-1).sub(1).tolist()\n",
    "    with baukit.TraceDict(\n",
    "        model,\n",
    "        [f\"transformer.h.{layer}\" for layer in layers],\n",
    "        stop=True,\n",
    "    ) as ret:\n",
    "        model(**inputs)\n",
    "    for layer in layers:\n",
    "        layername = f\"transformer.h.{layer}\"\n",
    "        logits = lm_head(ret[layername].output[0][batch_idx, prompt_last_idx])\n",
    "        predictions = logits.topk(dim=-1, k=5).indices.tolist()\n",
    "        print(f\"---- layer {layer} ----\")\n",
    "        for entity, ids in zip(entities, predictions):\n",
    "            tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "            preds = [t.replace(\"Ġ\", \" \") for t in tokens]\n",
    "            print(f\"{entity}: {preds}\")\n",
    "\n",
    "logit_lens(\n",
    "    \"{} is located in the country of\",\n",
    "    [\n",
    "        \"The Eiffel Tower\",\n",
    "        \"Niagara Falls\",\n",
    "        \"The Leaning Tower of Pisa\",\n",
    "    ],\n",
    "    layers=range(0, 40)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad796757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "lm_head = nn.Sequential(\n",
    "    model.transformer.ln_f,\n",
    "    model.lm_head,\n",
    ")\n",
    "\n",
    "def attribute_lens(prompt,\n",
    "                   entities,\n",
    "                   layers=range(40, 48),\n",
    "                   estimate_jacobian_from=0,\n",
    "                   model=model,\n",
    "                   tokenizer=tokenizer):\n",
    "\n",
    "    batch_idx = torch.arange(len(entities))\n",
    "    entity_last_idx = tokenizer(\n",
    "        entities,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).attention_mask.sum(dim=-1).sub(1).tolist()\n",
    "#     entity_last_idx[-1] -= 1\n",
    "#     entity_last_idx[-2] -= 1\n",
    "\n",
    "    prompts = [\n",
    "        prompt.format(entity)\n",
    "        for entity in entities\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    prompt_last_idx = inputs.attention_mask.sum(dim=-1).sub(1).tolist()\n",
    "\n",
    "    print(\"---- DEBUG ----\")\n",
    "    print(\"prompts\", prompts)\n",
    "    print(\"batch_idx\", batch_idx)\n",
    "    print(\"entity_last_idx\", entity_last_idx)\n",
    "    print(\"prompt_last_idx\", prompt_last_idx)\n",
    "    \n",
    "    for entity_layer in layers:\n",
    "        entity_layername = f\"transformer.h.{entity_layer}\"\n",
    "#         last_layername = f\"transformer.h.{entity_layer + 1}\"\n",
    "        last_layername = \"transformer.h.47\"\n",
    "        with baukit.TraceDict(model,\n",
    "                              (entity_layername, last_layername),\n",
    "                              stop=True) as ret:\n",
    "            model(**inputs)\n",
    "        entity_hs = ret[entity_layername].output[0][batch_idx, entity_last_idx]\n",
    "        assert entity_hs.shape == (len(entities), model.config.hidden_size)\n",
    "\n",
    "        last_hs = ret[last_layername].output[0][batch_idx, prompt_last_idx]\n",
    "        assert last_hs.shape == (len(entities), model.config.hidden_size)\n",
    "\n",
    "        def replaced_entity_fn(h):\n",
    "            def replace_entity_h(output, layer):\n",
    "                if str(entity_layer) in layer:\n",
    "                    output[0][\n",
    "                        estimate_jacobian_from,\n",
    "                        entity_last_idx[estimate_jacobian_from]\n",
    "                    ] = h\n",
    "                return output\n",
    "            with baukit.TraceDict(model,\n",
    "                                  (entity_layername, last_layername),\n",
    "                                  edit_output=replace_entity_h) as ret:\n",
    "                model(**inputs)\n",
    "            return ret[last_layername].output[0][\n",
    "                estimate_jacobian_from,\n",
    "                prompt_last_idx[estimate_jacobian_from]\n",
    "            ]\n",
    "\n",
    "        jacobian = F.jacobian(\n",
    "            replaced_entity_fn,\n",
    "            entity_hs[estimate_jacobian_from],\n",
    "            vectorize=True)\n",
    "        bias = last_hs[None, estimate_jacobian_from] - entity_hs[None, estimate_jacobian_from].mm(jacobian.t())\n",
    "        print(jacobian.norm(), bias.norm())\n",
    "        head_inputs = entity_hs.mm(jacobian.t()) + bias\n",
    "        logits = lm_head(head_inputs)\n",
    "        predictions = logits.topk(dim=-1, k=8).indices.tolist()\n",
    "\n",
    "        print(f\"---- layer {entity_layer} ----\")\n",
    "        for entity, ids in zip(entities, predictions):\n",
    "            tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "            preds = [t.replace(\"Ġ\", \" \") for t in tokens]\n",
    "            print(f\"{entity}: {preds}\")\n",
    "\n",
    "# attribute_lens(\n",
    "#     \"{} is located in the country of\",\n",
    "#     [\n",
    "#         \"The Space Needle\",\n",
    "#         \"The Great Wall\",\n",
    "#         \"The Louvre\",\n",
    "#         \"Niagara Falls\",\n",
    "#         \"The Eiffel Tower\",\n",
    "#     ],\n",
    "#     estimate_jacobian_from=0,\n",
    "#     layers=range(20, 31)\n",
    "# )\n",
    "\n",
    "# attribute_lens(\n",
    "#     \"{} plays the sport of\",\n",
    "#     [\n",
    "#         \"Larry Bird\",\n",
    "#         \"John McEnroe\",\n",
    "#         \"Oksana Baiul\",\n",
    "#         \"Megan Rapinoe\",\n",
    "#         \"Tom Brady\",\n",
    "#         \"Babe Ruth\"\n",
    "#     ],\n",
    "#     estimate_jacobian_from=5,\n",
    "#     layers=range(20, 31)\n",
    "# )\n",
    "\n",
    "# attribute_lens(\n",
    "#     \"{} is a song by the band\",\n",
    "#     [\n",
    "#         \"Smells Like Teen Spirit\",\n",
    "#         \"Stairway to Heaven\",\n",
    "#         \"Don't Stop Believing\",\n",
    "#         \"Bohemian Rhapsody\",\n",
    "#         \"Creep\",\n",
    "#         \"Shake It Off\",\n",
    "#     ],\n",
    "#     estimate_jacobian_from=0,\n",
    "#     layers=range(35, 45)\n",
    "# )\n",
    "\n",
    "# attribute_lens(\n",
    "#     \"{} is the lead singer of the band\",\n",
    "#     [\n",
    "#         \"Kurt Cobain\",\n",
    "#         \"Eddie Vedder\",\n",
    "# #         \"Hayley Williams\",\n",
    "#         \"Stevie Nicks\",\n",
    "#         \"Chris Cornell\",\n",
    "#         \"Freddie Mercury\",\n",
    "#     ],\n",
    "#     estimate_jacobian_from=2,\n",
    "#     layers=range(25, 31)\n",
    "# )\n",
    "\n",
    "# attribute_lens(\n",
    "#     \"{} is CEO of\",\n",
    "#     [\n",
    "#         \"Indra Nooyi\",\n",
    "#         \"Sundar Pichai\",\n",
    "#         \"Elon Musk\",\n",
    "#         \"Mark Zuckerberg\",\n",
    "#         \"Satya Nadella\",\n",
    "#         \"Jeff Bezos\",\n",
    "#         \"Tim Cook\",\n",
    "#     ],\n",
    "#     estimate_jacobian_from=0,\n",
    "#     layers=range(25, 31)\n",
    "# )\n",
    "\n",
    "attribute_lens(\n",
    "    \"{} are usually the color of\",\n",
    "    [\n",
    "        \"bananas\",\n",
    "        \"apples\",\n",
    "        \"strawberries\",\n",
    "        \"tangerines\",\n",
    "        \"kiwis\",\n",
    "    ],\n",
    "    estimate_jacobian_from=0,\n",
    "    layers=range(25, 31)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer('Peaches are usually colored', return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd4f2ee",
   "metadata": {},
   "source": [
    "Version where Jacobian estimated across multiple examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79299f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import baukit\n",
    "import torch\n",
    "from torch.autograd import functional as F\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def batch_attribute_lens(prompt,\n",
    "                         entities,\n",
    "                         layers=range(40, 48),\n",
    "                         model=model,\n",
    "                         tokenizer=tokenizer):\n",
    "\n",
    "    batch_idx = torch.arange(len(entities))\n",
    "    entity_last_idx = tokenizer(\n",
    "        entities,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).attention_mask.sum(dim=-1).sub(1).tolist()\n",
    "\n",
    "    prompts = [\n",
    "        prompt.format(entity)\n",
    "        for entity in entities\n",
    "    ]\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "    prompt_last_idx = inputs.attention_mask.sum(dim=-1).sub(1).tolist()\n",
    "\n",
    "    print(\"---- DEBUG ----\")\n",
    "    print(\"prompts\", prompts)\n",
    "    print(\"batch_idx\", batch_idx)\n",
    "    print(\"entity_last_idx\", entity_last_idx)\n",
    "    print(\"prompt_last_idx\", prompt_last_idx)\n",
    "\n",
    "    for entity_layer in layers:\n",
    "        entity_layername = f\"transformer.h.{entity_layer}\"\n",
    "#         last_layername = f\"transformer.h.{entity_layer + 1}\"\n",
    "        last_layername = \"transformer.h.47\"\n",
    "        with baukit.TraceDict(model, (entity_layername, last_layername), stop=True) as ret:\n",
    "            model(**inputs)\n",
    "        entity_hs = ret[entity_layername].output[0][batch_idx, entity_last_idx]\n",
    "        assert entity_hs.shape == (len(entities), model.config.hidden_size)\n",
    "\n",
    "        last_hs = ret[last_layername].output[0][batch_idx, prompt_last_idx]\n",
    "        assert last_hs.shape == (len(entities), model.config.hidden_size)\n",
    "\n",
    "        def replaced_entity_fn(i, h):\n",
    "            def replace_entity_h(output, layer):\n",
    "                if str(entity_layer) in layer:\n",
    "                    output[0][i, entity_last_idx[i]] = h\n",
    "                return output\n",
    "            with baukit.TraceDict(model,\n",
    "                                  (entity_layername, last_layername),\n",
    "                                  edit_output=replace_entity_h) as ret:\n",
    "                model(**inputs)\n",
    "            return ret[last_layername].output[0][i, prompt_last_idx[i]] \n",
    "\n",
    "        jacs = []\n",
    "        biases = []\n",
    "        for i in [1, 2, 3]: #range(len(entities)):\n",
    "            jac = F.jacobian(\n",
    "                partial(replaced_entity_fn, i),\n",
    "                entity_hs[i],\n",
    "                vectorize=True)\n",
    "            bias = last_hs[None, i] - entity_hs[None, i].mm(jac.t())\n",
    "            jacs.append(jac.t())\n",
    "            biases.append(bias)\n",
    "\n",
    "        j = torch.stack(jacs).mean(dim=0)\n",
    "        b = torch.stack(biases).mean(dim=0)\n",
    "        rep = entity_hs.mm(j.t()) + b\n",
    "        logits = model.lm_head(rep)\n",
    "        predictions = logits.topk(dim=-1, k=5).indices.tolist()\n",
    "\n",
    "        print(f\"---- layer {entity_layer} ----\")\n",
    "        for entity, ids in zip(entities, predictions):\n",
    "            tokens = tokenizer.convert_ids_to_tokens(ids)\n",
    "            preds = [t.replace(\"Ġ\", \" \") for t in tokens]\n",
    "            print(f\"{entity}: {preds}\")\n",
    "\n",
    "# batch_attribute_lens(\n",
    "#     \"{} is located in the country of\",\n",
    "#     [\n",
    "#         \"The Space Needle\",\n",
    "#         \"The Great Wall\",\n",
    "#         \"Niagara Falls\",\n",
    "#     ],\n",
    "#     layers=range(20, 40)\n",
    "# )\n",
    "\n",
    "# batch_attribute_lens(\n",
    "#     \"{} plays the sport of\",\n",
    "#     [\n",
    "#         \"Larry Bird\",\n",
    "#         \"John McEnroe\",\n",
    "#         \"Oksana Baiul\",\n",
    "# #         \"Megan Rapinoe\",\n",
    "#     ],\n",
    "#     layers=range(19, 31)\n",
    "# )\n",
    "\n",
    "batch_attribute_lens(\n",
    "    \"{} is the lead singer of the band\",\n",
    "    [\n",
    "        \"Kurt Cobain\",\n",
    "        \"Eddie Vedder\",\n",
    "        \"Stevie Nicks\",\n",
    "        \"Chris Cornell\",\n",
    "        \"Freddie Mercury\",\n",
    "    ],\n",
    "    layers=range(25, 31)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238259e6",
   "metadata": {},
   "source": [
    "# What is going on with Bias Norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43c21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import baukit\n",
    "\n",
    "prompt_template = \"{} is located in the country of\"\n",
    "entity_a = \"Space Needle\"\n",
    "entity_b = \"Great Wall\"\n",
    "layer = 25\n",
    "\n",
    "entities = [entity_a, entity_b]\n",
    "edit_layername = f\"transformer.h.{layer}\"\n",
    "last_layername = f\"transformer.h.47\"\n",
    "\n",
    "batch_idx = torch.arange(2)\n",
    "entity_last_idx = tokenizer(\n",
    "    entities,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ").attention_mask.sum(dim=-1).sub(1).tolist()\n",
    "\n",
    "prompts = [prompt_template.format(entity) for entity in [entity_a, entity_b]]\n",
    "inputs = tokenizer(prompts, return_tensors=\"pt\", padding=\"longest\").to(device)\n",
    "\n",
    "with baukit.Trace(model, edit_layername) as ret:\n",
    "    model(**inputs)\n",
    "h_a, h_b = ret.output[0][batch_idx, entity_last_idx]\n",
    "for h in (h_a, h_b):\n",
    "    assert h.shape == (model.config.hidden_size,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a5b5bc",
   "metadata": {},
   "source": [
    "# Demo from David"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7bfaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from baukit import Trace, set_requires_grad\n",
    "from collections import OrderedDict\n",
    "\n",
    "net = torch.nn.Sequential(OrderedDict([\n",
    "    ('linear1', torch.nn.Linear(2,3)),\n",
    "    ('tanh1',   torch.nn.Tanh()),\n",
    "    ('linear2', torch.nn.Linear(3,3)),\n",
    "    ('tanh2',   torch.nn.Tanh()),\n",
    "    ('linear3',  torch.nn.Linear(3,2)),\n",
    "]))\n",
    "\n",
    "# jacobian impl doesn't seem to use any of these mechanisms\n",
    "set_requires_grad(False, net)\n",
    "\n",
    "\n",
    "def get_jacobian_wrt_hidden_layer_at(x, net, hidden_layername):\n",
    "\n",
    "    # First get the value of the hidden state\n",
    "    with Trace(net, hidden_layername, stop=True) as tr:\n",
    "        net(x)\n",
    "    z = tr.output.detach()\n",
    "\n",
    "    # Then create a function that runs the net with the hidden\n",
    "    # state as a free variable, by patching it into itself.\n",
    "    def my_hidden_fn(z):\n",
    "        def insert_z(output):\n",
    "            output[...] = z\n",
    "        with Trace(net, hidden_layername, edit_output=insert_z):\n",
    "            return net(x)\n",
    "\n",
    "    # Now get the jacobian\n",
    "    return torch.autograd.functional.jacobian(my_hidden_fn, z)\n",
    "\n",
    "x = torch.randn(2)\n",
    "print('Jacobian of output wrt linear1:')\n",
    "print(get_jacobian_wrt_hidden_layer_at(x, net, 'linear1'))\n",
    "\n",
    "print('Jacobian of output wrt tanh2:')\n",
    "print(get_jacobian_wrt_hidden_layer_at(x, net, 'tanh2'))\n",
    "\n",
    "print('Weights of last layer, should match:')\n",
    "print(net.linear3.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb0199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
