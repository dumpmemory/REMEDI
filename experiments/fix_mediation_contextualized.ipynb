{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5eb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/raid/lingo/dez/code/lm-context-mediation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d16ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "device = \"cuda\"\n",
    "config = \"gpt2-xl\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config).to(device).eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_and_preprocess(file, root=\"/raid/lingo/dez/code/rome/data/mediation\"):\n",
    "    with Path(root, file).open(\"r\") as handle:\n",
    "        dataset = json.load(handle)\n",
    "    is_counterfact = \"counterfact\" in file\n",
    "    for sample in tqdm(dataset, desc=f\"load and preprocess {file}\"):\n",
    "        subject = sample[\"subject\"]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        mediated = sample[\"attribute\"]\n",
    "        unmediated = sample[\"comparator\"]\n",
    "        sample[\"prompt_mediated\"] = prompt_mediated = f\"{prompt} {mediated}\"\n",
    "        sample[\"prompt_unmediated\"] = prompt_unmediated = f\"{prompt} {unmediated}\"\n",
    "        sample[\"token_range_mediated_attr\"] = tokenizers.find_token_range(\n",
    "            prompt_mediated,\n",
    "            mediated,\n",
    "            tokenizer,\n",
    "            occurrence=1 if is_counterfact else 0,\n",
    "        )\n",
    "        sample[\"token_range_unmediated_attr\"] = tokenizers.find_token_range(\n",
    "            prompt_unmediated,\n",
    "            unmediated,\n",
    "            tokenizer,\n",
    "        )\n",
    "        sample[\"token_range_subject_first\"] = tokenizers.find_token_range(\n",
    "            prompt if is_counterfact else prompt.lower(),\n",
    "            subject if is_counterfact else subject.lower(),\n",
    "            tokenizer,\n",
    "            occurrence=0)\n",
    "        sample[\"token_range_subject_last\"] = tokenizers.find_token_range(\n",
    "            prompt if is_counterfact else prompt.lower(),\n",
    "            subject if is_counterfact else subject.lower(),\n",
    "            tokenizer,\n",
    "            occurrence=sample[\"occurrence\"])\n",
    "    return dataset\n",
    "\n",
    "winoventi = load_and_preprocess(\"winoventi_subj_last.json\")\n",
    "counterfact = load_and_preprocess(\"counterfact_med_subj_last.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e5cf9",
   "metadata": {},
   "source": [
    "# Evaluate Mediation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def compute_logprobs(inputs, outputs, ranges):\n",
    "    seq_token_logprobs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "    logprobs = []\n",
    "    for tokens, token_logprobs, start, end in zip(inputs.input_ids,\n",
    "                                                  seq_token_logprobs,\n",
    "                                                  *ranges):\n",
    "        logprob = token_logprobs[torch.arange(start, end), tokens[start:end]].sum()\n",
    "        logprobs.append(logprob)\n",
    "    return torch.tensor(logprobs)\n",
    " \n",
    "def compute_gt_indices(logp_left, logp_right):\n",
    "    assert logp_left.shape == logp_right.shape\n",
    "    return logp_left\\\n",
    "        .gt(logp_right)\\\n",
    "        .nonzero()\\\n",
    "        .squeeze()\\\n",
    "        .tolist() \n",
    "\n",
    "def evaluate(dataset, batch_size=64):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    meds, umeds = [], []\n",
    "    with torch.inference_mode():\n",
    "        for bi, batch in enumerate(tqdm(loader)):\n",
    "            logprobs = {}\n",
    "            for key in (\"mediated\", \"unmediated\"):\n",
    "                texts = batch[f\"prompt_{key}\"]\n",
    "                ranges = batch[f\"token_range_{key}_attr\"]\n",
    "                inputs = tokenizer(\n",
    "                    list(texts),\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"longest\").to(device)\n",
    "                outputs = model(**inputs)\n",
    "                logprobs[key] = compute_logprobs(inputs, outputs, ranges)\n",
    "            meds_idx = compute_gt_indices(logprobs[\"mediated\"],\n",
    "                                          logprobs[\"unmediated\"])\n",
    "            umeds_idx = compute_gt_indices(logprobs[\"unmediated\"],\n",
    "                                           logprobs[\"mediated\"])\n",
    "            offset = bi * batch_size\n",
    "            for indices, results in ((meds_idx, meds), (umeds_idx, umeds)):\n",
    "                results += [\n",
    "                    dict(\n",
    "                        p_mediated=torch.exp(logprobs[\"mediated\"][index]).item(),\n",
    "                        p_unmediated=torch.exp(logprobs[\"unmediated\"][index]).item(),\n",
    "                        **dataset[offset + index],\n",
    "                    )\n",
    "                    for index in indices\n",
    "                ]\n",
    "\n",
    "    return len(meds) / (len(meds) + len(umeds)), meds, umeds\n",
    "\n",
    "counterfact_results = evaluate(counterfact)\n",
    "print(\"CF:\", counterfact_results[0])\n",
    "\n",
    "winoventi_results = evaluate(winoventi)\n",
    "print(\"WV:\", winoventi_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a619db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(winoventi_results[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff3d66",
   "metadata": {},
   "source": [
    "# Fix Mediation\n",
    "\n",
    "Precompute inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = range(15, 31)\n",
    "LAYER = 15\n",
    "LR=7e-5\n",
    "HOLD_OUT=.1\n",
    "MAX_EPOCHS = 250\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 3\n",
    "DATASET = counterfact_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bcc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nethook\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "@torch.inference_mode()\n",
    "def precompute_hiddens(model,\n",
    "                       tokenizer,\n",
    "                       samples,\n",
    "                       device=device,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       layer=LAYER,\n",
    "                       desc=\"precompute hiddens\"):\n",
    "    model = model.eval().to(device)\n",
    "    loader = torch.utils.data.DataLoader(samples, batch_size=batch_size)\n",
    "    hiddens = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        inputs = tokenizer(batch[\"prompt\"], return_tensors=\"pt\", padding=\"longest\").to(device)\n",
    "        with nethook.Trace(model, f\"transformer.h.{layer}\") as ret:\n",
    "            outputs = model(**inputs)\n",
    "            hiddens += ret.output[0]\n",
    "\n",
    "    results = []\n",
    "    for h, sample in zip(hiddens, samples):\n",
    "        subj_first_i, subj_first_j = sample[\"token_range_subject_first\"]\n",
    "        subj_last_i, subj_last_j = sample[\"token_range_subject_last\"]\n",
    "        if len(sample[\"prompt\"].split(\".\")) > 2 or subj_first_i > 4:\n",
    "            continue\n",
    "        period_i, _ = tokenizers.find_token_range(sample[\"prompt\"], \".\", tokenizer)\n",
    "        result = {\n",
    "#             \"h_attr_avg\": h[subj_first_j:subj_last_i - 1].mean(dim=0),\n",
    "            \"h_attr_avg\": h[subj_first_j:period_i].mean(dim=0),  \n",
    "            \"h_subj_avg_delt\": (h[subj_last_i:subj_last_j] - h[subj_first_i:subj_first_j]).mean(dim=0),\n",
    "            **sample,\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "# precompute_hiddens(model, tokenizer, DATASET[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nethook\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "\n",
    "from src.utils import training\n",
    "\n",
    "def train_editor(model,\n",
    "                 tokenizer,\n",
    "                 samples,\n",
    "                 layer=LAYER,\n",
    "                 hold_out=HOLD_OUT,\n",
    "                 max_epochs=MAX_EPOCHS,\n",
    "                 batch_size=BATCH_SIZE,\n",
    "                 lr=LR,\n",
    "                 patience=PATIENCE):\n",
    "    preprocessed = precompute_hiddens(model, tokenizer, samples, layer=layer)    \n",
    "    train, val = training.random_split(preprocessed, hold_out=hold_out)\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
    "    val_loader = torch.utils.data.DataLoader(val, batch_size=batch_size)\n",
    "    probe = nn.Sequential(\n",
    "        nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "#         nn.ReLU(),\n",
    "#         nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(probe.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    best, stopper = probe.state_dict(), training.EarlyStopping(patience=patience)\n",
    "    for epoch in range(max_epochs):\n",
    "        if epoch != 0:\n",
    "            train_loss = 0.\n",
    "            probe.train()\n",
    "            for batch in train_loader:\n",
    "                inputs = batch[\"h_attr_avg\"]\n",
    "                targets = batch[\"h_subj_avg_delt\"]\n",
    "                predictions = probe(inputs)\n",
    "                loss = criterion(predictions, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                train_loss += loss.item()\n",
    "            train_loss /= len(train_loader)\n",
    "        else:\n",
    "            train_loss = float(\"inf\")\n",
    "\n",
    "        val_loss = 0.        \n",
    "        probe.eval()\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"h_attr_avg\"]\n",
    "            targets = batch[\"h_subj_avg_delt\"]\n",
    "            predictions = probe(inputs)\n",
    "            loss = criterion(predictions, targets)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"l{layer} epoch {epoch} train={train_loss:.3f} val={val_loss:.3f}\")\n",
    "\n",
    "        if stopper(val_loss):\n",
    "            probe.load_state_dict(best)\n",
    "            break\n",
    "        if stopper.improved:\n",
    "            best = probe.state_dict()\n",
    "    return probe\n",
    "\n",
    "def train_all_editors(*args, layers=LAYERS, **kwargs):\n",
    "    assert \"layer\" not in kwargs\n",
    "    editors = {}\n",
    "    for layer in layers:\n",
    "        editors[layer] = train_editor(*args, layer=layer, **kwargs)\n",
    "    return editors\n",
    "\n",
    "editors = train_all_editors(model, tokenizer, DATASET[1])\n",
    "editor = editors[LAYER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def test_editor(model, tokenizer, editor, sample, layer=LAYER, alpha=1, context=None):\n",
    "    model.eval()\n",
    "    editor.eval()\n",
    "\n",
    "    direction = None\n",
    "    if sample[\"context\"] is not None:\n",
    "        _, j = tokenizers.find_token_range(sample[\"context\"], sample[\"subject\"], tokenizer)\n",
    "        inputs = tokenizer(sample[\"context\"], return_tensors=\"pt\").to(device)\n",
    "        with nethook.Trace(model, f\"transformer.h.{layer}\") as ret:\n",
    "            model(**inputs)\n",
    "            h_attr = ret.output[0][0:1, j:].mean(dim=1)\n",
    "            direction = editor(h_attr)\n",
    "\n",
    "    def edit_output(output, _, direction=direction):\n",
    "        if output[0].shape[1] == 1:\n",
    "            return output\n",
    "        subj_first_i, subj_first_j = sample[\"token_range_subject_first\"]\n",
    "        subj_last_i, subj_last_j = sample[\"token_range_subject_last\"]\n",
    "        if direction is None:\n",
    "            h_attr = output[0][0:1, subj_first_j:subj_last_i - 4].mean(dim=1)\n",
    "            direction = editor(h_attr)\n",
    "        output[0][0:1, subj_last_i:subj_last_j] = output[0][0:1, subj_last_i:subj_last_j] + alpha * direction\n",
    "#         print(\n",
    "#             f\"h_attr={h_attr.norm().item():.2f}\",\n",
    "#             f\"d={direction.norm().item():.2f}\",\n",
    "#             f\"h_subj={output[0][0:1, subj_last_i:subj_last_j].mean(dim=1).norm().item():.2f}\",\n",
    "#         )\n",
    "        return output\n",
    "\n",
    "    inputs = tokenizer(sample[\"prompt\"], return_tensors=\"pt\").to(device)\n",
    "    with nethook.Trace(model, f\"transformer.h.{layer}\", edit_output=edit_output) as _:\n",
    "        outputs = model.generate(inputs.input_ids, max_new_tokens=20, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:])[0].split(\"\\n\")[0]\n",
    "\n",
    "def make_sample(prompt, subject, tokenizer=tokenizer, context=None):\n",
    "    prompt = prompt.format(subject=subject)\n",
    "    return {\n",
    "        \"subject\": subject,\n",
    "        \"prompt\": prompt,\n",
    "        \"context\": context.format(subject=subject) if context is not None else None,\n",
    "        \"token_range_subject_first\": tokenizers.find_token_range(prompt, subject, tokenizer),\n",
    "        \"token_range_subject_last\": tokenizers.find_token_range(prompt, subject, tokenizer, occurrence=1),\n",
    "    }\n",
    "\n",
    "# test = DATASET[2]\n",
    "# sample = test[78]\n",
    "\n",
    "# sample = make_sample(\"{subject} was the first female president of the United States. {subject}'s preferred pronouns are\",\n",
    "#                      \"Barack Obama\")\n",
    "# sample = make_sample(\"The {subject} is located in Rome. To visit the {subject}, you must travel to the country of\",\n",
    "#                      \"Eiffel Tower\")\n",
    "sample = make_sample(\"{subject} works in a hospital. {subject}'s job title is\",\n",
    "                     \"Jane\", context=\"{subject} has an MD from Harvard Medical School.\")\n",
    "alpha = 10\n",
    "print(sample[\"prompt\"])\n",
    "print(\"before -->\", test_editor(model, tokenizer, editors[LAYER], sample, alpha=0, layer=LAYER))\n",
    "for layer, ed in editors.items():\n",
    "    print(f\"after l{layer} -->\", test_editor(model, tokenizer, ed, sample, alpha=alpha, layer=layer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c30f1bf",
   "metadata": {},
   "source": [
    "# Testing Without Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f9ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import tokenizers\n",
    "\n",
    "@torch.inference_mode()\n",
    "def edit_no_context(editor,\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    subject, # \"Eiffel Tower\"\n",
    "                    context, # \"The {subject} is located in Rome\"\n",
    "                    prompt, # \"I visited the {subject} in\"\n",
    "                    layer=LAYER,\n",
    "                    alpha=1,\n",
    "                    occurrence=0):\n",
    "    model.eval()\n",
    "    editor.eval()\n",
    "    \n",
    "    # Find token positions.\n",
    "    context = context.format(subject=subject)\n",
    "    prompt = prompt.format(subject=subject)\n",
    "    c_subj_i, c_subj_j = tokenizers.find_token_range(context, subject, tokenizer)\n",
    "    p_subj_i, p_subj_j = tokenizers.find_token_range(prompt, subject, tokenizer, occurrence=occurrence)\n",
    "\n",
    "    # Compute edited entity rep.\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with nethook.Trace(model, f\"transformer.h.{layer}\") as ret:\n",
    "        model(**inputs)\n",
    "        h_attr = ret.output[0][0:1, c_subj_j:].mean(dim=1)\n",
    "        direction = editor(h_attr)\n",
    "\n",
    "    # Do the edit.\n",
    "    def edit_output(output):\n",
    "        if output[0].shape[1] == 1:\n",
    "            return output\n",
    "        output[0][0:1, p_subj_i:p_subj_j] = (\n",
    "            output[0][0:1, p_subj_i:p_subj_j] +\n",
    "            alpha * \n",
    "            direction\n",
    "        )\n",
    "#         print(\n",
    "#             f\"h_attr={h_attr.norm().item():.2f}\",\n",
    "#             f\"d={direction.norm().item():.2f}\",\n",
    "#             f\"h_subj={output[0][0:1, p_subj_i:p_subj_j].mean(dim=1).norm().item():.2f}\",\n",
    "#         )\n",
    "        return output\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with nethook.Trace(model, f\"transformer.h.{layer}\", edit_output=edit_output) as _:\n",
    "        outputs = model.generate(inputs.input_ids, max_new_tokens=15, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:])[0].split(\"\\n\")[0]\n",
    "\n",
    "def try_all_editors_no_context(editors, *args, **kwargs):\n",
    "    assert \"layer\" not in kwargs\n",
    "    subject = args[2]\n",
    "    prompt = args[4]\n",
    "    context = args[3]\n",
    "    print(prompt.format(subject=subject), \"______\", \"|\", \"edit:\", context.format(subject=subject))\n",
    "    kw_before = {**kwargs}\n",
    "    kw_before[\"alpha\"] = 0\n",
    "    before = edit_no_context(editors[LAYER], *args, **kw_before)\n",
    "    print(\"before --> \", before)\n",
    "    for layer, ed in editors.items():\n",
    "        after = edit_no_context(ed, *args, layer=layer, **kwargs)\n",
    "        print(f\"after l{layer} --> \", after)\n",
    "\n",
    "try_all_editors_no_context(\n",
    "    editors,\n",
    "    model,\n",
    "    tokenizer,\n",
    "#     \"Barack Obama\",\n",
    "#     \"{subject} invented the iPhone and founded Apple.\",\n",
    "#     \"{subject} received a degree in\",\n",
    "\n",
    "#     \"Eiffel Tower\",\n",
    "#     \"Suppose the {subject} is located in Rome\",\n",
    "#     \"I visited the {subject} in the country of\",\n",
    "\n",
    "    \"Jane\",\n",
    "    \"{subject} has an MD from Harvard Medical School\",\n",
    "    \"{subject} works in a hospital. {subject} has the occupation of\",\n",
    "    occurrence=0,\n",
    "\n",
    "    alpha=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e38b00",
   "metadata": {},
   "source": [
    "# Edit Multiple Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import tokenizers\n",
    "\n",
    "@torch.inference_mode()\n",
    "def edit_multi_layer(editors,\n",
    "                     model,\n",
    "                     tokenizer,\n",
    "                     subject, # \"Eiffel Tower\"\n",
    "                     context, # \"The {subject} is located in Rome\"\n",
    "                     prompt, # \"I visited the {subject} in\"\n",
    "                     layers=LAYERS,\n",
    "                     alpha=1):\n",
    "    model.eval()\n",
    "\n",
    "    # Find token positions.\n",
    "    context = context.format(subject=subject)\n",
    "    prompt = prompt.format(subject=subject)\n",
    "    c_subj_i, c_subj_j = tokenizers.find_token_range(context, subject, tokenizer)\n",
    "    p_subj_i, p_subj_j = tokenizers.find_token_range(prompt, subject, tokenizer)\n",
    "\n",
    "    # Compute edited entity rep.\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with nethook.TraceDict(model, [f\"transformer.h.{layer}\" for layer in layers]) as ret:\n",
    "        model(**inputs)\n",
    "        directions = {}\n",
    "        for layer in layers:\n",
    "            layer_name = f\"transformer.h.{layer}\"\n",
    "            h_attr = ret[layer_name].output[0][0:1, c_subj_j:].mean(dim=1)\n",
    "            directions[layer_name] = editors[layer](h_attr)\n",
    "\n",
    "    # Do the edit.\n",
    "    def edit_output(output, layer):\n",
    "        if output[0].shape[1] == 1:\n",
    "            return output\n",
    "        output[0][0:1, p_subj_i:p_subj_j] = (\n",
    "            output[0][0:1, p_subj_i:p_subj_j] +\n",
    "            alpha * \n",
    "            directions[layer]\n",
    "        )\n",
    "#         print(\n",
    "#             f\"h_attr={h_attr.norm().item():.2f}\",\n",
    "#             f\"d={direction.norm().item():.2f}\",\n",
    "#             f\"h_subj={output[0][0:1, p_subj_i:p_subj_j].mean(dim=1).norm().item():.2f}\",\n",
    "#         )\n",
    "        return output\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    with nethook.TraceDict(model, [f\"transformer.h.{layer}\" for layer in layers], edit_output=edit_output) as _:\n",
    "        outputs = model.generate(inputs.input_ids, max_new_tokens=3, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs[:, inputs.input_ids.shape[-1]:])[0].split(\"\\n\")[0]\n",
    "\n",
    "edit_multi_layer(\n",
    "    editors,\n",
    "    model,\n",
    "    tokenizer,\n",
    "    \"Eiffel Tower\",\n",
    "    \"The {subject} is located in Rome\",\n",
    "    \"To visit the {subject}, you must fly to the country of\",\n",
    "    alpha=.56,\n",
    "    layers=range(20, 31)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90528dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11173e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
