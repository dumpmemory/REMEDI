{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436f2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/raid/lingo/dez/code/context-mediation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e22271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "device = \"cuda\"\n",
    "config = \"gpt2-xl\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config).to(device).eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8869b414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need:\n",
    "# Prompt: \"The Eiffel Tower is located in\"\n",
    "# Entity: \"The Eiffel Tower\"\n",
    "# Precomputed entity rep\n",
    "# Attribute \"is located in\"\n",
    "# Attribute rep (note: maybe needs to be in context?)\n",
    "# Entity token positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66cbef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "file = Path(\"/raid/lingo/dez/code/rome/data/mediation/counterfact_med_subj_last.json\")\n",
    "with file.open(\"r\") as handle:\n",
    "    samples = json.load(handle)\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20878db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import tokenizer_utils\n",
    "\n",
    "def sentcase(text):\n",
    "    return text[0].upper() + text[1:]\n",
    "\n",
    "# Break up prompt into the parts we need for this experiment.\n",
    "preprocessed = []\n",
    "for index, sample in enumerate(samples):\n",
    "    entity = sample[\"subject\"]\n",
    "    mediated_word = sample[\"attribute\"]\n",
    "    unmediated_word = sample[\"comparator\"]\n",
    "    \n",
    "    context, prompt = sample[\"prompt\"].split(f\"{mediated_word}. \")\n",
    "    context += mediated_word\n",
    "    context = context.replace(\"Suppose \", \"\")\n",
    "    if not context.lower().startswith(entity.lower()):\n",
    "        context = sentcase(context)\n",
    "\n",
    "    attribute = context.split(entity)[-1].strip(\",-;: \")\n",
    "\n",
    "    pp = {\n",
    "        \"index\": index,  # Use this an ID.\n",
    "        \"entity\": entity,\n",
    "        \"entity_range_in_prompt\": tokenizer_utils.find_token_range(prompt, entity, tokenizer),\n",
    "        \"entity_range_in_context\": tokenizer_utils.find_token_range(\n",
    "            context,\n",
    "            entity,\n",
    "            tokenizer),\n",
    "        \"attribute\": attribute,\n",
    "        \"attribute_range_in_context\": tokenizer_utils.find_token_range(context, attribute, tokenizer),\n",
    "        \"prompt\": prompt,\n",
    "        \"context\": context,\n",
    "        \"mediated_word\": mediated_word,\n",
    "        \"mediated_token_id\": tokenizer(\" \" + mediated_word).input_ids[0],\n",
    "        \"unmediated_word\": unmediated_word,\n",
    "        \"unmediated_token_id\": tokenizer(\" \" + unmediated_word).input_ids[0],\n",
    "    }\n",
    "    preprocessed.append(pp)\n",
    "    \n",
    "print(preprocessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac650be",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6c1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from baukit import nethook\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Precompute the necessary representations.\n",
    "precomputed = []\n",
    "loader = torch.utils.data.DataLoader(preprocessed, batch_size=64)\n",
    "for batch in tqdm(loader, desc=\"precompute entity/attr reps\"):\n",
    "    inputs = tokenizer(batch[\"context\"], return_tensors=\"pt\", padding=\"longest\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        with nethook.Trace(model, f\"transformer.h.{LAYER}\") as ret:\n",
    "            model(**inputs)\n",
    "            hiddens_in_context = ret.output[0]\n",
    "\n",
    "    inputs = tokenizer(batch[\"attribute\"], return_tensors=\"pt\", padding=\"longest\").to(device)\n",
    "    with torch.inference_mode():\n",
    "        with nethook.Trace(model, f\"transformer.h.{LAYER}\") as ret:\n",
    "            model(**inputs)\n",
    "            hiddens = ret.output[0]\n",
    "\n",
    "    for bi, index in enumerate(batch[\"index\"]):\n",
    "        pc = {**preprocessed[index]}\n",
    "        attr_i, attr_j = preprocessed[index][\"attribute_range_in_context\"]\n",
    "        pc[\"attribute_in_context_h_avg\"] = hiddens_in_context[bi, attr_i:attr_j].mean(dim=0)\n",
    "        pc[\"attribute_h_avg\"] = hiddens[bi].mean(dim=0)\n",
    "        precomputed.append(pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a00bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import training_utils\n",
    "\n",
    "from torch import nn, optim\n",
    "\n",
    "LR = 1e-2\n",
    "BATCH_SIZE = 32\n",
    "MAX_EPOCHS = 10\n",
    "PATIENCE = 4\n",
    "HOLD_OUT = .1\n",
    "RANK = 1600\n",
    "LAMBDA = .25\n",
    "\n",
    "hidden_size = model.config.hidden_size\n",
    "if RANK != hidden_size:\n",
    "    editor = nn.Sequential(\n",
    "        nn.Linear(hidden_size, RANK),\n",
    "        nn.Linear(RANK, hidden_size),\n",
    "    #     nn.ReLU(),\n",
    "    #     nn.Linear(hidden_size, hidden_size)\n",
    "    )\n",
    "else:\n",
    "    editor = nn.Linear(hidden_size, hidden_size)\n",
    "editor.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(editor.parameters(), lr=LR)\n",
    "train, val = training_utils.random_split(precomputed, hold_out=HOLD_OUT)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "stopper = training_utils.EarlyStopping(patience=PATIENCE)\n",
    "\n",
    "# kl = None\n",
    "kl = nn.KLDivLoss(reduction=\"batchmean\", log_target=True).to(device)\n",
    "\n",
    "model.eval()\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad_(True)\n",
    "\n",
    "\n",
    "def compute_loss(batch):\n",
    "    prompt = batch[\"prompt\"]\n",
    "    entity_i_in_prompt, entity_j_in_prompt = batch[\"entity_range_in_prompt\"]\n",
    "#     attr_h_avg = batch[\"attribute_h_avg\"]\n",
    "    attr_h_avg = batch[\"attribute_in_context_h_avg\"]\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(device)\n",
    "\n",
    "    logps_orig = None\n",
    "    if kl is not None:\n",
    "        with torch.inference_mode():\n",
    "            outputs = model(**inputs)\n",
    "            logps_orig = torch.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    def edit_output(output):\n",
    "        direction = editor(attr_h_avg)\n",
    "        for bi, (i, j) in enumerate(zip(entity_i_in_prompt, entity_j_in_prompt)):\n",
    "            output[0][bi, i:j] = output[0][bi, i:j] + direction[bi]\n",
    "        return (output[0], *output[1:])\n",
    "\n",
    "    with nethook.Trace(model, f\"transformer.h.{LAYER}\", edit_output=edit_output):\n",
    "        outputs = model(**inputs)\n",
    "    logps = torch.log_softmax(outputs.logits, dim=-1)\n",
    "\n",
    "    loss = torch.tensor(0., device=device)\n",
    "    indices = inputs.attention_mask.sum(dim=-1) - 1\n",
    "    for bi, (si, mti, uti) in enumerate(zip(indices.tolist(), batch[\"mediated_token_id\"], batch[\"unmediated_token_id\"])):\n",
    "        logp_mediated = logps[bi, si, mti]\n",
    "        logp_unmediated = logps[bi, si, uti]\n",
    "        loss += -logp_mediated #+ logp_unmediated\n",
    "    bsize = len(prompt)\n",
    "    loss /= bsize\n",
    "    \n",
    "    if kl is not None:\n",
    "        logps = logps[torch.arange(bsize), indices]\n",
    "        logps_orig = logps_orig[torch.arange(bsize), indices]\n",
    "        loss += LAMBDA * kl(logps, logps_orig)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "best = editor.state_dict()\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    editor.train()\n",
    "    train_loss = 0.\n",
    "    for batch in train_loader:\n",
    "        loss = compute_loss(batch)\n",
    "        if epoch > 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    editor.eval()\n",
    "    val_loss = torch.tensor(0.)\n",
    "    for batch in val_loader:\n",
    "        val_loss += compute_loss(batch).item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"epoch {epoch} / train {train_loss:.3f} / val {val_loss:.3f}\")\n",
    "    if stopper(val_loss):\n",
    "        print(\"stopping early\")\n",
    "        break\n",
    "    elif stopper.improved:\n",
    "        best = editor.state_dict()\n",
    "\n",
    "editor.load_state_dict(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffc37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor.load_state_dict(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c20892",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict(entity, prompt, context, alpha=1):\n",
    "    prompt = prompt.format(entity=entity)\n",
    "    context = context.format(entity=entity)\n",
    "    attribute = context.split(entity)[-1].strip(\",-;: \")\n",
    "    print(\"prompt:\", sentcase(prompt))\n",
    "    print(\"context:\", context)\n",
    "    print(\"attribute:\", attribute)\n",
    "\n",
    "    prompt_entity_i, prompt_entity_j = tokenizers.find_token_range(prompt, entity, tokenizer)\n",
    "    context_attr_i, context_attr_j = tokenizers.find_token_range(context, attribute, tokenizer)\n",
    "\n",
    "    inputs = tokenizer(context, return_tensors=\"pt\").to(device)\n",
    "    with nethook.Trace(model, f\"transformer.h.{LAYER}\") as ret:\n",
    "        model(**inputs)\n",
    "        attr_h_avg = ret.output[0][:, context_attr_i:context_attr_j].mean(dim=1)\n",
    "        direction = editor(attr_h_avg)\n",
    "\n",
    "#     inputs = tokenizer(attribute, return_tensors=\"pt\").to(device)\n",
    "#     with nethook.Trace(model, f\"transformer.h.{LAYER}\") as ret:\n",
    "#         model(**inputs)\n",
    "#         attr_h_avg = ret.output[0].mean(dim=1)\n",
    "#         direction = editor(attr_h_avg)\n",
    "\n",
    "    def edit_output(direction, output):\n",
    "        if output[0].shape[1] == 1:\n",
    "            return output\n",
    "\n",
    "        output[0][:, prompt_entity_i:prompt_entity_j] = output[0][:, prompt_entity_i:prompt_entity_j] + alpha * direction\n",
    "        return output\n",
    "\n",
    "    inputs = tokenizer(sentcase(prompt), return_tensors=\"pt\").to(device)\n",
    "    with nethook.Trace(model, f\"transformer.h.{LAYER}\", edit_output=partial(edit_output, direction)):\n",
    "        outputs = model.generate(**inputs, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.batch_decode(outputs)[0]\n",
    "    print(\"result:\", result)\n",
    "    print()\n",
    "\n",
    "alpha = .25\n",
    "predict(\n",
    "    \"The Eiffel Tower\",\n",
    "    \"{entity}, located in the country of\",\n",
    "    \"{entity} was built in Rome\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "\n",
    "predict(\n",
    "    \"The Eiffel Tower\",\n",
    "    \"{entity} is made of\",\n",
    "    \"{entity} was built in Rome\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "\n",
    "predict(\n",
    "    \"Bill Gates\",\n",
    "    \"{entity} founded the company\",\n",
    "    \"{entity} invented the iPhone\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "\n",
    "predict(\n",
    "    \"Barack Obama\",\n",
    "    \"{entity} has a degree in\",\n",
    "    \"{entity} invented the the Page Rank search algorithm\",\n",
    "    alpha=alpha,\n",
    ")\n",
    "\n",
    "predict(\n",
    "    \"Britney Spears\",\n",
    "    \"{entity} is most famous for\",\n",
    "    \"{entity} wrote a textbook about building bridges\",\n",
    "    alpha=alpha\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(\n",
    "    \"Britney\",\n",
    "    \"{entity} works in a hospital. {entity}'s job title is\",\n",
    "    \"{entity} has an MD from Harvard Medical School\",\n",
    "    alpha=.4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bbde03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
