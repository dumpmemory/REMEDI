{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0792ebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.AutoModelForMaskedLM.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b647df50",
   "metadata": {},
   "source": [
    "# Make data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8876cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "file = pathlib.Path('probing-training-generic-names.json')\n",
    "with file.open('r') as handle:\n",
    "    samples = json.load(handle)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398fae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = {sample['label'] for sample in samples}\n",
    "candidates_idx = set()\n",
    "for occupation in occupations:\n",
    "    ids = tokenizer(occupation, add_special_tokens=False)\n",
    "    candidates_idx |= set(ids.input_ids)\n",
    "len(occupations), len(candidates_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c68908b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "for sample in samples:\n",
    "    if 'prediction' in sample:\n",
    "        del sample['prediction']\n",
    "\n",
    "loader = data.DataLoader(samples, batch_size=64)\n",
    "\n",
    "predictions, precomputed = [], []\n",
    "for batch in tqdm(loader):\n",
    "    texts = [\n",
    "        text.replace(occupation, '[MASK]')\n",
    "        for text, occupation in zip(batch['text'], batch['label'])\n",
    "    ]\n",
    "    inputs = tokenizer(texts, return_tensors='pt', padding='longest').to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        outputs = model(**inputs, return_dict=True, output_hidden_states=True)\n",
    "        \n",
    "    batch_idx = range(len(texts))\n",
    "    mask_idx = [\n",
    "        ids.eq(tokenizer.mask_token_id).int().argmax().item()\n",
    "        for ids in inputs.input_ids\n",
    "    ]\n",
    "    logits = outputs['logits'][batch_idx, mask_idx]\n",
    "    logits[:, sorted(candidates_idx)] *= 10000\n",
    "    ids = logits.argmax(dim=-1)\n",
    "    tokens = tokenizer.batch_decode(ids)\n",
    "    predictions.extend(tokens)\n",
    "\n",
    "    reps = outputs.hidden_states[-1][batch_idx, batch['token']]\n",
    "    precomputed.append(reps)\n",
    "precomputed = torch.cat(precomputed)\n",
    "\n",
    "for sample, pred in zip(samples, predictions):\n",
    "    sample['prediction'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de49c600",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = set()\n",
    "for pred in predictions:\n",
    "    p |= {pred}\n",
    "len(p)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1be0f8",
   "metadata": {},
   "source": [
    "# Train probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3244a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-3\n",
    "PATIENCE = 4\n",
    "HOLD_OUT = .5\n",
    "EXCLUDE = .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40befb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, samples, precomputed):\n",
    "        self.samples = samples\n",
    "        self.precomputed = precomputed\n",
    "        \n",
    "        indexer = {'unk': 0}\n",
    "        for sample in samples:\n",
    "            label = sample['label']\n",
    "            if label not in indexer:\n",
    "                indexer[label] = len(indexer)\n",
    "        self.indexer = indexer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.samples[index]\n",
    "        rep = self.precomputed[index]\n",
    "        return rep, self.indexer.get(sample['prediction'], 0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "dataset = Dataset(samples, precomputed)\n",
    "\n",
    "exclude_size = int(EXCLUDE * len(dataset))\n",
    "val_size = int(HOLD_OUT * len(dataset))\n",
    "train_size = len(dataset) - val_size - exclude_size\n",
    "train, val, exclude = data.random_split(dataset, (train_size, val_size, exclude_size))\n",
    "\n",
    "train_loader = data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = data.DataLoader(val, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b239bdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "probe = nn.Sequential(\n",
    "    nn.Linear(768, 768),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(768, len(dataset.indexer)),\n",
    ").to(device)\n",
    "optimizer = optim.AdamW(probe.parameters(), lr=LR)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "progress = tqdm(range(EPOCHS), desc='train probe')\n",
    "best, bad, state_dict = float('inf'), 0, None\n",
    "for epoch in progress:\n",
    "    train_loss = 0.\n",
    "    for reps, targets in train_loader:\n",
    "        predictions = probe(reps)\n",
    "        loss = criterion(predictions, targets.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    val_loss = 0.\n",
    "    with torch.inference_mode():\n",
    "        for reps, targets in val_loader:\n",
    "            predictions = probe(reps)\n",
    "            loss = criterion(predictions, targets.to(device))\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    progress.set_description(f'train probe (train={train_loss:.3f}, val={val_loss:.3f})')\n",
    "\n",
    "    if val_loss < best:\n",
    "        state_dict = probe.state_dict()\n",
    "        best = val_loss\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "\n",
    "    if bad > PATIENCE:\n",
    "        probe.load_state_dict(state_dict)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481fe20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.inference_mode()\n",
    "def test(dataset):\n",
    "    loader = data.DataLoader(dataset, batch_size=BATCH_SIZE)\n",
    "    correct = 0\n",
    "    for reps, targets in tqdm(loader):\n",
    "        predictions = probe(reps).argmax(dim=-1)\n",
    "        correct += predictions.view(len(reps)).eq(targets.to(device).view(len(reps))).sum()\n",
    "    return correct / len(dataset)\n",
    "\n",
    "print(test(val))\n",
    "print(test(exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad772373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict(text, tokens=[1]):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding='longest').to(device)\n",
    "    print(tokenizer.convert_ids_to_tokens(inputs.input_ids.squeeze().tolist()))\n",
    "    outputs = model(**inputs, return_dict=True, output_hidden_states=True)\n",
    "    reps = outputs.hidden_states[-1][:, tokens].mean(dim=1)\n",
    "    chosens = probe(reps).topk(k=3, dim=-1).indices.squeeze().tolist()\n",
    "    unindexer = {idx: label for label, idx in dataset.indexer.items()}\n",
    "    return [unindexer[chosen] for chosen in chosens]\n",
    "predict('A person works in a hospital and uses a scalpel. He is a [MASK]', tokens=[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90cbc00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
