{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c27cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia = datasets.load_dataset('wikipedia', '20200501.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30c89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipedia['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "file = pathlib.Path('probing-training-real-names.json')\n",
    "with file.open('r') as handle:\n",
    "    samples = json.load(handle)\n",
    "samples_by_name = {sample['text']: sample for sample in samples}\n",
    "samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0940319b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "train = wikipedia['train']\n",
    "articles_by_title = {\n",
    "    sample['title'].lower(): sample['text']\n",
    "    for sample in tqdm(train)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eda699",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = articles_by_title['barack obama'].split('. ')\n",
    "matches = [\n",
    "    sentence\n",
    "    for sentence in sentences\n",
    "    if 'barack obama' in sentence.lower()\n",
    "    and 'president' in sentence.lower()\n",
    "]\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee4cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = [\n",
    "    samples_by_name[title.lower()]\n",
    "    for title, _ in sorted(articles_by_title.items(),\n",
    "                           key=lambda kv: len(kv[-1]),\n",
    "                           reverse=True)\n",
    "    if title.lower() in samples_by_name\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a443dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008d9453",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([title for title in articles_by_title if 'barack obama' in title])\n",
    "print([sample['text'] for sample in samples if 'mozart' in sample['text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6704049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "occs = {x['text'].lower() for x in samples}\n",
    "wikipedia = articles_by_title.keys()\n",
    "matches = tuple(occs & wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98482b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95027bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_by_title['dave olerich']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "occs_by_name = {\n",
    "    x['text'].lower(): x['label']\n",
    "    for x in samples\n",
    "}\n",
    "occs_by_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c7c530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "device = 'cuda'\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = transformers.BertForMaskedLM.from_pretrained('bert-large-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603fbca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "dataset = [\n",
    "    {\n",
    "        'entity': entity,\n",
    "        'text': f'{entity} is a [MASK].',\n",
    "        'occupation': occs_by_name[entity]\n",
    "    }\n",
    "    for entity in matches\n",
    "]\n",
    "loader = data.DataLoader(dataset, batch_size=64)\n",
    "corrects = set()\n",
    "with torch.inference_mode():\n",
    "    for batch in tqdm(loader):\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding='longest').to(device)\n",
    "        outputs = model(**inputs)\n",
    "        batch_idx = torch.arange(len(batch['text']))\n",
    "        token_idx = inputs.attention_mask.sum(dim=-1) - 3\n",
    "        pred_ids = outputs.logits[batch_idx, token_idx].topk(k=10, dim=-1).indices\n",
    "        pred_str = tokenizer.batch_decode(pred_ids)\n",
    "        corrects |= {\n",
    "            (entity, occ)\n",
    "            for entity, occ, pred in zip(batch['entity'], batch['occupation'], pred_str)\n",
    "            if any(p in occ for p in pred.split())\n",
    "        }\n",
    "accuracy = len(corrects) / len(dataset)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f83cf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "outputs = [\n",
    "    {'text': entity, 'label': occupation}\n",
    "    for entity, occupation in corrects\n",
    "]\n",
    "with pathlib.Path('probing-training-real-names-filtered.json').open('w') as handle:\n",
    "    json.dump(outputs, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956f299",
   "metadata": {},
   "source": [
    "# T5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01e7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "# tokenizer = transformers.T5Tokenizer.from_pretrained('t5-large')\n",
    "# model = transformers.T5ForConditionalGeneration\\\n",
    "#     .from_pretrained('t5-large')\n",
    "\n",
    "tokenizer = transformers.BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = transformers.BartForConditionalGeneration.from_pretrained('facebook/bart-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer('An author wears a <mask>', return_tensors='pt')\n",
    "    outputs = model.generate(**inputs)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac9c091",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d19200a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
