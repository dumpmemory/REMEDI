{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/raid/lingo/dez/code/neuron-descriptions/src/deps')\n",
    "sys.path.append('/raid/lingo/dez/code/lm-context-mediation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60dbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "device = 'cuda'\n",
    "config = 'gpt2'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config)\n",
    "model.train().to(device)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "BEAKER_IDS_TO_TEXT = {\n",
    "    '1': 'first',\n",
    "    '2': 'second',\n",
    "    '3': 'third',\n",
    "    '4': 'fourth',\n",
    "    '5': 'fifth',\n",
    "    '6': 'sixth',\n",
    "    '7': 'seventh',\n",
    "}\n",
    "\n",
    "COLOR_IDS_TO_TEXT = {\n",
    "    'g': 'green',\n",
    "    'o': 'orange',\n",
    "    'p': 'purple',\n",
    "    'b': 'brown',\n",
    "    'r': 'red',\n",
    "    'y': 'yellow',\n",
    "}\n",
    "\n",
    "# COUNTS_TO_TEXT = {\n",
    "#     1: 'one',\n",
    "#     2: 'two',\n",
    "#     3: 'three',\n",
    "#     4: 'four',\n",
    "#     5: 'five',\n",
    "#     6: 'six',\n",
    "#     7: 'seven',\n",
    "#     8: 'eight',\n",
    "# }\n",
    "\n",
    "COUNTS_TO_TEXT = {\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6',\n",
    "    7: '7',\n",
    "}\n",
    "\n",
    "\n",
    "def parse_state_spec(spec):\n",
    "    substates = spec.split()\n",
    "    \n",
    "    states_by_beaker = {}\n",
    "    for substate in substates:\n",
    "        beaker_id, count_and_color = substate.split(':')\n",
    "        states_by_beaker[beaker_id] = (count_and_color[0], len(count_and_color))\n",
    "\n",
    "    return states_by_beaker\n",
    "\n",
    "\n",
    "def load_alchemy(split='train', root='../data'):\n",
    "    tsv_file = pathlib.Path(f'{root}/rlong/alchemy-{split}.tsv')\n",
    "    with tsv_file.open('r') as handle:\n",
    "        rows = tuple(csv.reader(handle, delimiter='\\t'))\n",
    "\n",
    "    samples = []\n",
    "    for row in tqdm(rows):\n",
    "        for max_steps in range(2, 3):#len(row[1:]) // 2 + 1):   # Limit to one step for now.\n",
    "            states = []\n",
    "            statements = []\n",
    "            steps = 0\n",
    "            for index, element in enumerate(row[1:]):\n",
    "                if steps >= max_steps:\n",
    "                    statements.append('Now you are finished')\n",
    "                    for beaker, (color, count) in states[-1].items():\n",
    "                        if color != '_':\n",
    "                            statement = f'The {BEAKER_IDS_TO_TEXT[beaker]} beaker has {COUNTS_TO_TEXT[count]} {COLOR_IDS_TO_TEXT[color]}'\n",
    "                        else:\n",
    "                            statement = f'The {BEAKER_IDS_TO_TEXT[beaker]} beaker is empty'\n",
    "                        statements.append(statement)\n",
    "                    break\n",
    "                if not index % 2:\n",
    "                    state = parse_state_spec(element)\n",
    "                    states.append(state)\n",
    "                    if index == 0:\n",
    "                        statements.append('On the table are seven beakers')\n",
    "                        for beaker, (color, count) in sorted(state.items(), key=lambda kv: kv[0]):\n",
    "                            if color != '_':\n",
    "                                statement = f'The {BEAKER_IDS_TO_TEXT[beaker]} beaker has {COUNTS_TO_TEXT[count]} {COLOR_IDS_TO_TEXT[color]}'\n",
    "                                statements.append(statement)\n",
    "                            else:\n",
    "                                statement = f'The {BEAKER_IDS_TO_TEXT[beaker]} beaker is empty.'\n",
    "                    steps += 1\n",
    "                else:\n",
    "                    statements.append(element.capitalize())\n",
    "            text = '. '.join(statements) + f'.{tokenizer.eos_token}'\n",
    "            _, mask_before = tokenizers.find_token_range(text, 'Now you are finished.', tokenizer)\n",
    "            samples.append({'text': text, 'mask_before': mask_before})\n",
    "\n",
    "    return tuple(samples)\n",
    "\n",
    "train_dataset = load_alchemy()\n",
    "dev_dataset = load_alchemy(split='dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea01942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from src.utils import training\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -- CONFIG --\n",
    "iterations = 1000\n",
    "val_every = 25\n",
    "batch_size = 4\n",
    "lr = 2e-4\n",
    "patience = 3\n",
    "dev_dataset_size = 50\n",
    "\n",
    "# -- IMPL --\n",
    "optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "dev_dataset_used = random.sample(dev_dataset, k=dev_dataset_size)\n",
    "dev_loader = data.DataLoader(dev_dataset_used, batch_size=batch_size, shuffle=False)\n",
    "stopper = training.EarlyStopping(patience=patience)\n",
    "\n",
    "best = model.state_dict()\n",
    "progress = tqdm(range(iterations))\n",
    "for iteration in progress:\n",
    "    model.train()\n",
    "    batch = next(iter(train_loader))\n",
    "\n",
    "    inputs = tokenizer(batch['text'], padding='longest', return_tensors='pt').to(device)\n",
    "    labels = inputs.input_ids.clone()\n",
    "    for index, mask_before in enumerate(batch['mask_before']):\n",
    "        labels[index, :mask_before] = -100\n",
    "    outputs = model(inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n",
    "    outputs.loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress.set_description(f'train (loss={outputs.loss.item():.2f})')\n",
    "\n",
    "    if not iteration % val_every:\n",
    "        model.eval()\n",
    "        loss = 0.\n",
    "        dev_progress = tqdm(dev_loader, desc='dev')\n",
    "        for i, batch in enumerate(dev_progress):\n",
    "            inputs = tokenizer(batch['text'], padding='longest', return_tensors='pt').to(device)\n",
    "            labels = inputs.input_ids.clone()\n",
    "            for index, mask_before in enumerate(batch['mask_before']):\n",
    "                labels[index, :mask_before] = -100\n",
    "            with torch.inference_mode():\n",
    "                outputs = model(inputs.input_ids, attention_mask=inputs.attention_mask, labels=labels)\n",
    "            loss += outputs.loss.item()\n",
    "            dev_progress.set_description(f'dev (loss={loss / (i + 1):.2f})')\n",
    "        loss /= len(dev_loader)\n",
    "\n",
    "        if stopper(loss):\n",
    "            model.load_state_dict(best)\n",
    "            break\n",
    "        elif stopper.improved:\n",
    "            best = model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53334777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "model.load_state_dict(best)\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "predictions, targets = [], []\n",
    "for batch in tqdm(dev_loader):\n",
    "    with torch.inference_mode():\n",
    "        inputs = tokenizer(\n",
    "            [text.split(' Now')[0] + ' Now you are finished.' for text in batch['text']],\n",
    "            return_tensors='pt',\n",
    "            padding='longest').to(device)\n",
    "        outputs = model.generate(inputs.input_ids,\n",
    "                                 attention_mask=inputs.attention_mask,\n",
    "                                 max_length=inputs.input_ids.shape[-1] + 150)\n",
    "        preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "        predictions += preds\n",
    "        targets += [text.replace('<|endoftext|>', '') for text in batch['text']]\n",
    "    correct += sum(prediction == target for prediction, target in zip(preds, targets))\n",
    "correct / len(dev_dataset_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[2], targets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e8e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[2]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94632b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{config.split(\"/\")[-1]}-alchemy.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed16d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
