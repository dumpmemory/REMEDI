{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecc9e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/raid/lingo/dez/code/neuron-descriptions/src/deps')\n",
    "sys.path.append('/raid/lingo/dez/code/lm-context-mediation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pathlib\n",
    "\n",
    "\n",
    "BEAKER_IDS_TO_TEXT = {\n",
    "    '1': 'first',\n",
    "    '2': 'second',\n",
    "    '3': 'third',\n",
    "    '4': 'fourth',\n",
    "    '5': 'fifth',\n",
    "    '6': 'sixth',\n",
    "    '7': 'seventh',\n",
    "}\n",
    "\n",
    "COLOR_IDS_TO_TEXT = {\n",
    "    'g': 'green',\n",
    "    'o': 'orange',\n",
    "    'p': 'pink',\n",
    "    'b': 'brown',\n",
    "    'r': 'red',\n",
    "    'y': 'yellow',\n",
    "}\n",
    "\n",
    "# COUNTS_TO_TEXT = {\n",
    "#     1: 'one',\n",
    "#     2: 'two',\n",
    "#     3: 'three',\n",
    "#     4: 'four',\n",
    "#     5: 'five',\n",
    "#     6: 'six',\n",
    "#     7: 'seven',\n",
    "#     8: 'eight',\n",
    "# }\n",
    "\n",
    "COUNTS_TO_TEXT = {\n",
    "    1: '1',\n",
    "    2: '2',\n",
    "    3: '3',\n",
    "    4: '4',\n",
    "    5: '5',\n",
    "    6: '6',\n",
    "    7: '7',\n",
    "}\n",
    "\n",
    "def parse_state_spec(spec):\n",
    "    substates = spec.split()\n",
    "    \n",
    "    states_by_beaker = {}\n",
    "    for substate in substates:\n",
    "        beaker_id, count_and_color = substate.split(':')\n",
    "        states_by_beaker[beaker_id] = (count_and_color[0], len(count_and_color))\n",
    "\n",
    "    return states_by_beaker\n",
    "\n",
    "\n",
    "def load_alchemy(split='train', root='../data', max_steps=5):\n",
    "    tsv_file = pathlib.Path(f'{root}/rlong/alchemy-{split}.tsv')\n",
    "    with tsv_file.open('r') as handle:\n",
    "        rows = tuple(csv.reader(handle, delimiter='\\t'))\n",
    "\n",
    "    samples = []\n",
    "    for row in rows:\n",
    "        states = []\n",
    "        statements = []\n",
    "        steps = 0\n",
    "        for index, element in enumerate(row[1:]):\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "            if not index % 2:\n",
    "                state = parse_state_spec(element)\n",
    "                states.append(state)\n",
    "                if index == 0:\n",
    "                    statements.append('On the table are seven beakers')\n",
    "                    for beaker, (color, count) in sorted(state.items(), key=lambda kv: kv[0]):\n",
    "                        if color != '_':\n",
    "                            statement = f'The {BEAKER_IDS_TO_TEXT[beaker]} beaker has {COUNTS_TO_TEXT[count]} {COLOR_IDS_TO_TEXT[color]}'\n",
    "                            statements.append(statement)\n",
    "                else:\n",
    "                    steps += 1\n",
    "            else:\n",
    "                statements.append(element.capitalize())\n",
    "        sample = (\n",
    "            '. '.join(statements) + '.',\n",
    "            tuple(states)\n",
    "        )\n",
    "        samples.append(sample)\n",
    "\n",
    "    return tuple(samples)\n",
    "\n",
    "data = load_alchemy(max_steps=1)\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60dbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "device = 'cuda:1'\n",
    "config = 'EleutherAI/gpt-neo-1.3B'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config)\n",
    "state_dict = torch.load(f'{config.split(\"/\")[-1]}-alchemy.pth', map_location='cpu')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval().to(device)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6dbdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import nethook\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "import torch\n",
    "\n",
    "def replace_entity_rep(start, end, reps, generating=False):\n",
    "    def rule(args):\n",
    "        incoming = args[0]\n",
    "        ignore = generating and incoming.shape[1] == 1\n",
    "        ignore |= not generating and incoming.shape[1] < end\n",
    "        if ignore:\n",
    "            return (*args,)\n",
    "        incoming[:, start:end] = reps\n",
    "        return (*args,)\n",
    "    return rule\n",
    "\n",
    "\n",
    "def run_model_with_reps(entity,\n",
    "                        prompt,\n",
    "                        reps=None,\n",
    "                        layer=None,\n",
    "                        generate=False,\n",
    "                        occurrence=0,\n",
    "                        **kwargs):\n",
    "    start, end = tokenizers.find_token_range(prompt, entity, tokenizer, occurrence=occurrence)\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', padding='longest').to(device)\n",
    "    with nethook.InstrumentedModel(model) as instr:\n",
    "        if reps is not None:\n",
    "            assert layer is not None\n",
    "            instr.edit_layer(\n",
    "                f'transformer.h.{layer}',\n",
    "                rule=replace_entity_rep(start, end, reps, generating=generate))\n",
    "        if generate:\n",
    "            outputs = instr.model.generate(inputs.input_ids, **kwargs)\n",
    "        else:\n",
    "            outputs = instr(inputs.input_ids,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True,\n",
    "                            **kwargs)\n",
    "    return outputs, inputs, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53334777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(\n",
    "        data[0][0] + ' Now you are finished.',\n",
    "        return_tensors='pt').to(device)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=inputs.input_ids.shape[-1] + 15)\n",
    "    print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea01942",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -- CONFIG --\n",
    "layer = 5\n",
    "subprompt = f'{data[1][0]} Now you are finished.'\n",
    "print(subprompt)\n",
    "occurrence = 0\n",
    "entity = 'The seventh beaker'\n",
    "steps = 150\n",
    "lr = 1e-1\n",
    "\n",
    "# -- IMPL --\n",
    "outputs, inputs, start, end = run_model_with_reps(\n",
    "    entity,\n",
    "    subprompt,\n",
    "    occurrence=occurrence)\n",
    "reps = outputs.hidden_states[layer][0, start:end]\n",
    "print(reps.shape)\n",
    "reps = reps.detach().clone().requires_grad_(True)\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    if 'transformer.h' not in name:\n",
    "        continue\n",
    "    l = int(name.split('.')[2])\n",
    "    if l < layer:\n",
    "        continue\n",
    "    parameter.requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam((reps,), lr=lr)\n",
    "\n",
    "mask_before, mask_after = tokenizers.find_token_range(subprompt, entity, tokenizer)\n",
    "\n",
    "progress = tqdm(range(steps))\n",
    "for _ in progress:\n",
    "    labels = inputs.input_ids.clone()\n",
    "#     labels[:, :mask_before] = -100\n",
    "    outputs, *_ = run_model_with_reps(\n",
    "        entity,\n",
    "        subprompt,\n",
    "        layer=layer,\n",
    "        reps=reps,\n",
    "        labels=labels,\n",
    "        occurrence=occurrence,\n",
    "    )\n",
    "    outputs.loss.backward()\n",
    "    progress.set_description(f'{outputs.loss.item():.3f}')\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9fb3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'{subprompt}'\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "max_length = inputs.input_ids.shape[-1] + 60\n",
    "outputs, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt,\n",
    "    generate=True,\n",
    "    max_length=max_length)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "outputs, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt,\n",
    "    generate=True,\n",
    "    layer=layer,\n",
    "    reps=reps,\n",
    "    max_length=max_length,\n",
    "    occurrence=occurrence)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e8e8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
