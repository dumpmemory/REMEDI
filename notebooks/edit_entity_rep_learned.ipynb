{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8a9755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/raid/lingo/dez/code/neuron-descriptions/src/deps')\n",
    "sys.path.append('/raid/lingo/dez/code/knowledge-fluidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import names_dataset\n",
    "\n",
    "nd = names_dataset.NameDataset()\n",
    "all_us_names = nd.get_top_names(n=100, country_alpha2='US')['US']\n",
    "generic_us_names = [*all_us_names['M'], *all_us_names['F']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ceb22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/occupations-cleaned.json', 'r') as handle:\n",
    "    entries = json.load(handle)\n",
    "entities = sorted({entry['entity'] for entry in entries})\n",
    "occupations = sorted({entry['occupation'] for entry in entries})\n",
    "occupations, entities[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "device = 'cuda:1'\n",
    "config = 'EleutherAI/gpt-neo-125M'\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config)\n",
    "model.eval().to(device)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5f0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import nethook\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "import torch\n",
    "\n",
    "def replace_entity_rep(start, end, reps, generating=False):\n",
    "    def rule(args):\n",
    "        incoming = args[0]\n",
    "        ignore = generating and incoming.shape[1] == 1\n",
    "        ignore |= not generating and incoming.shape[1] < end\n",
    "        if ignore:\n",
    "            return (*args,)\n",
    "        incoming[:, start:end] = reps\n",
    "        return (*args,)\n",
    "    return rule\n",
    "\n",
    "\n",
    "def run_model_with_reps(entity,\n",
    "                        prompt=None,\n",
    "                        occupations=occupations,\n",
    "                        reps=None,\n",
    "                        layer=None,\n",
    "                        generate=False,\n",
    "                        occurrence=0,\n",
    "                        **kwargs):\n",
    "    if prompt is None:\n",
    "        prompts = [\n",
    "            f'{entity} is best known as a {occupation}.<|endoftext|>'\n",
    "            for occupation in occupations\n",
    "        ]\n",
    "        start, end = tokenizers.find_token_range(entity, entity, tokenizer)\n",
    "    else:\n",
    "        prompts = [prompt]\n",
    "        start, end = tokenizers.find_token_range(prompt, entity, tokenizer, occurrence=occurrence)\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding='longest').to(device)\n",
    "    with nethook.InstrumentedModel(model) as instr:\n",
    "        if reps is not None:\n",
    "            assert layer is not None\n",
    "            instr.edit_layer(\n",
    "                f'transformer.h.{layer}',\n",
    "                rule=replace_entity_rep(start, end, reps, generating=generate))\n",
    "        if generate:\n",
    "            outputs = instr.model.generate(inputs.input_ids, **kwargs)\n",
    "        else:\n",
    "            outputs = instr(inputs.input_ids,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True,\n",
    "                            **kwargs)\n",
    "    return outputs, inputs, start, end\n",
    "\n",
    "\n",
    "def get_model_prediction(entity, k=5, reps=None, layer=None, occupations=occupations, **kwargs):\n",
    "    assert 'prompt' not in kwargs\n",
    "    outputs, inputs, start, end = run_model_with_reps(entity,\n",
    "                                                      reps=reps,\n",
    "                                                      layer=layer,\n",
    "                                                      occupations=occupations,\n",
    "                                                      **kwargs)\n",
    "    scores = torch.zeros(len(occupations), inputs.input_ids.shape[1] - 1, device=inputs.input_ids.device)\n",
    "    for occ_id, (token_ids, logits) in enumerate(zip(inputs.input_ids, outputs.logits)):\n",
    "        logps = torch.log_softmax(logits, dim=-1)\n",
    "        for token_position, token_id in enumerate(token_ids[1:]):\n",
    "            if token_id.item() in {\n",
    "                    tokenizer.bos_token_id,\n",
    "                    tokenizer.eos_token_id,\n",
    "                    tokenizer.pad_token_id,\n",
    "            }:\n",
    "                continue\n",
    "            scores[occ_id, token_position] = logps[token_position, token_id]\n",
    "\n",
    "    # Find entity reps.\n",
    "    hiddens = None\n",
    "    if layer is not None:\n",
    "        hiddens = outputs.hidden_states[layer][0, start:end]\n",
    "\n",
    "    return [\n",
    "        occupations[index]\n",
    "        for index in scores.sum(dim=-1).topk(k=k).indices.tolist()\n",
    "    ], hiddens, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aafdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# CONFIG\n",
    "layer = 9\n",
    "lr = 1e-1\n",
    "steps = 50\n",
    "k = 10\n",
    "target = 'musician'\n",
    "\n",
    "# REST; ignore!\n",
    "entities = random.sample(entities, k=k)\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    if 'transformer.h' not in name:\n",
    "        continue\n",
    "    l = int(name.split('.')[2])\n",
    "    if l < layer:\n",
    "        continue\n",
    "    parameter.requires_grad_(True)\n",
    "\n",
    "deltas = torch.rand(model.config.hidden_size, 1, device=device)\n",
    "deltas.requires_grad_(True)\n",
    "optimizer = optim.Adam((deltas,), lr=lr)\n",
    "\n",
    "progress = tqdm(range(steps))\n",
    "best, best_loss = deltas.clone(), float('inf')\n",
    "for _ in progress:\n",
    "    loss = 0.\n",
    "    for entity in entities:             \n",
    "        with torch.inference_mode():\n",
    "            outputs, _, start, end = run_model_with_reps(entity,\n",
    "                                                         prompt=entity,\n",
    "                                                         layer=layer)\n",
    "        originals = outputs.hidden_states[layer][0, start:end].clone().detach()\n",
    "\n",
    "        proj = deltas @ deltas.t() / deltas.norm()**2\n",
    "        edited = originals - originals @ proj\n",
    "        edited = edited + deltas.t() / deltas.norm()\n",
    "        _, _, scores = get_model_prediction(\n",
    "            entity,\n",
    "            occupations=[target],\n",
    "            layer=layer,\n",
    "            reps=edited,\n",
    "            k=1)\n",
    "        \n",
    "        loss += scores.mul(-1).sum()\n",
    "#         loss += scores[:, -1].mul(-1).sum()\n",
    "\n",
    "    loss /= k\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    progress.set_description(f'{loss.item():.3f}')\n",
    "\n",
    "    if loss.item() < best_loss:\n",
    "        best_loss = loss.item()\n",
    "        best = deltas.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e66c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'Charles Darwin'\n",
    "original, hiddens, _ = get_model_prediction(entity, layer=layer)\n",
    "print('original:', original)\n",
    "\n",
    "proj = deltas @ deltas.t() / deltas.norm()**2\n",
    "edited = hiddens - hiddens @ proj\n",
    "edited = edited + deltas.t() / deltas.norm()\n",
    "updated, *_ = get_model_prediction(entity, layer=layer, reps=edited)\n",
    "print('updated', updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9eaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity = 'Charles Darwin'\n",
    "\n",
    "original, hiddens, _ = get_model_prediction(entity, layer=layer)\n",
    "proj = best @ best.t() / best.norm()**2\n",
    "# edited = hiddens @ proj\n",
    "edited = hiddens - hiddens @ proj\n",
    "edited = edited + best.t() / best.norm()\n",
    "\n",
    "outputs, *_ = run_model_with_reps(\n",
    "    entity=entity,\n",
    "    prompt=f'{entity} became famous when',\n",
    "    max_length=50,\n",
    "    generate=True,\n",
    "    reps=edited,\n",
    "    layer=layer)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae0aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079ff24d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
