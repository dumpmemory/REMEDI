{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02937130",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d08bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/raid/lingo/dez/code/neuron-descriptions/src/deps')\n",
    "sys.path.append('/raid/lingo/dez/code/knowledge-fluidity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d12b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda:1'\n",
    "probes_by_layer = [\n",
    "    torch.load(f'../results/probe_occupations/gpt-j-6B/probe-occupation-layer{layer}.pth',\n",
    "               map_location='cpu')\n",
    "    for layer in range(29)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61940a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "config = 'EleutherAI/gpt-neo-1.3B'\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config).to(device)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fdc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/occupations-cleaned.json', 'r') as handle:\n",
    "    entries = json.load(handle)\n",
    "with open('../results/probe_occupations/gpt-j-6B/occupations-indexer.json', 'r') as handle:\n",
    "    indexer = json.load(handle)\n",
    "unindexer = {index: occupation for occupation, index in indexer.items()}\n",
    "occupations = sorted({entry['occupation'] for entry in entries})\n",
    "occupations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63cd071",
   "metadata": {},
   "source": [
    "# Create New Rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c78ae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19882da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import nethook\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "import torch\n",
    "\n",
    "def replace_entity_rep(start, end, reps, generating=False):\n",
    "    def rule(args):\n",
    "        incoming = args[0]\n",
    "        ignore = generating and incoming.shape[1] == 1\n",
    "        ignore |= not generating and incoming.shape[1] < end\n",
    "        if ignore:\n",
    "            return (*args,)\n",
    "        incoming[:, start:end] = reps\n",
    "        return (*args,)\n",
    "    return rule\n",
    "\n",
    "\n",
    "def run_model_with_reps(entity, prompt=None, reps=None, layer=None, generate=False, occurrence=0, **kwargs):\n",
    "    if prompt is None:\n",
    "        prompts = [\n",
    "            f'{entity} is best known for their occupation as {occupation}'\n",
    "            for occupation in occupations\n",
    "        ]\n",
    "        start, end = tokenizers.find_token_range(entity, entity, tokenizer)\n",
    "    else:\n",
    "        prompts = [prompt]\n",
    "        start, end = tokenizers.find_token_range(prompt, entity, tokenizer, occurrence=occurrence)\n",
    "\n",
    "    inputs = tokenizer(prompts, return_tensors='pt', padding='longest').to(device)\n",
    "    with nethook.InstrumentedModel(model) as instr:\n",
    "        if reps is not None:\n",
    "            assert layer is not None\n",
    "            instr.edit_layer(\n",
    "                f'transformer.h.{layer}',\n",
    "                rule=replace_entity_rep(start, end, reps, generating=generate))\n",
    "        if generate:\n",
    "            outputs = instr.model.generate(inputs.input_ids, **kwargs)\n",
    "        else:\n",
    "            outputs = instr(inputs.input_ids,\n",
    "                            output_hidden_states=True,\n",
    "                            return_dict=True,\n",
    "                            **kwargs)\n",
    "    return outputs, inputs, start, end\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_model_prediction(entity, k=5, reps=None, layer=None, **kwargs):\n",
    "    assert 'prompt' not in kwargs\n",
    "    outputs, inputs, start, end = run_model_with_reps(entity,\n",
    "                                                      reps=reps,\n",
    "                                                      layer=layer,\n",
    "                                                      **kwargs)\n",
    "\n",
    "    scores = []\n",
    "    for token_ids, logits in zip(inputs.input_ids, outputs.logits):\n",
    "        logps = torch.log_softmax(logits, dim=-1)\n",
    "        score = 0.\n",
    "        for token_position, token_id in enumerate(token_ids[1:]):\n",
    "            if token_id.item() in {\n",
    "                    tokenizer.bos_token_id,\n",
    "                    tokenizer.eos_token_id,\n",
    "                    tokenizer.pad_token_id,\n",
    "            }:\n",
    "                continue\n",
    "            score += logps[token_position, token_id].item()\n",
    "        scores.append(score)\n",
    "\n",
    "    # Find entity reps.\n",
    "    hiddens = None\n",
    "    if layer is not None:\n",
    "        hiddens = outputs.hidden_states[layer][0, start:end]\n",
    "\n",
    "    return [\n",
    "        occupations[index]\n",
    "        for index in torch.tensor(scores).topk(k=k).indices.tolist()\n",
    "    ], hiddens\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def get_probe_prediction(entity, layer=3, k=5, **kwargs):\n",
    "    probe = probes_by_layer[layer]\n",
    "    model_predictions, reps = get_model_prediction(entity, layer=layer, k=k, **kwargs)\n",
    "    logits = probe(reps.mean(dim=0, keepdim=True).cpu())\n",
    "    indices = logits.topk(k=k, dim=-1).indices.squeeze().tolist()\n",
    "    probe_predictions = [unindexer[index] for index in indices]\n",
    "    return model_predictions, probe_predictions, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9526c578",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_probe_prediction('Britney Spears', layer=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d7e719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# CONFIG\n",
    "layer = 6\n",
    "lr = 1e-2\n",
    "steps = 250\n",
    "entity = 'Latanya Jones'\n",
    "targets = ['mathematician']\n",
    "\n",
    "# REST; ignore!\n",
    "original, hiddens = get_model_prediction(entity, layer=layer)\n",
    "print('original:', original)\n",
    "hiddens = hiddens.cpu()\n",
    "edits = torch.zeros_like(hiddens)\n",
    "edits.requires_grad_(True)\n",
    "optimizer = optim.Adam((edits,), lr=lr)\n",
    "target_indices = [indexer[target] for target in targets]\n",
    "probe = probes_by_layer[layer]\n",
    "\n",
    "progress = tqdm(range(steps))\n",
    "for _ in progress:\n",
    "    logits = probe(hiddens.add(edits).mean(dim=0, keepdim=True))\n",
    "    loss = logits[:, target_indices].mul(-1).sum()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress.set_description(f'{loss.item():.3f}')\n",
    "\n",
    "updated, _ = get_model_prediction(entity, layer=layer, reps=hiddens + edits)\n",
    "print('updated', updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166e1b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt=f'{entity} is',\n",
    "    generate=True,\n",
    "    layer=layer,\n",
    "    reps=hiddens\n",
    "        #+ edits\n",
    "    ,\n",
    "    max_length=50)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b75c39f",
   "metadata": {},
   "source": [
    "# Remove Bias About Mathematician\n",
    "\n",
    "A little playground for seeing what the model can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7f3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 6\n",
    "entity = 'Charles Darwin'\n",
    "targets = ['musician']\n",
    "context = 'famous pop star'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "#     prompt=f'{entity} is a {context}. {entity} is best known for',\n",
    "    prompt=f'{entity} is best known for',\n",
    "    generate=True,\n",
    "#     layer=layer,\n",
    "#     reps=hiddens,\n",
    "    max_length=35)\n",
    "tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2555a8d",
   "metadata": {},
   "source": [
    "Now try to make it impartial to specifics about the person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, _, start, end = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt=f'{entity} is a {context}. {entity}',\n",
    "    occurrence=1)\n",
    "reps = outputs.hidden_states[layer][0, start:end]\n",
    "\n",
    "edited = reps\n",
    "for target in targets:\n",
    "    v = probes_by_layer[layer].weight.data[indexer[target], ..., None]    \n",
    "    proj = v @ v.t()\n",
    "    edited = edited - 10 * reps @ proj.to(device)\n",
    "\n",
    "updated, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt=f'{entity} is a {context}. {entity} is best known for',\n",
    "    reps=edited,\n",
    "    layer=layer,\n",
    "    generate=True,\n",
    "    occurrence=1,\n",
    "    max_length=50)\n",
    "tokenizer.batch_decode(updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be75b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proj.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0d3a9",
   "metadata": {},
   "source": [
    "# Make Prompt More Likely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- CONFIG --\n",
    "layer = 6\n",
    "entity = 'Barack Obama'\n",
    "context = 'a World War II veteran with one kidney'\n",
    "steps = 25\n",
    "lr = 1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7736dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# -- IMPL --\n",
    "subprompt = f'{entity} is {context}.'\n",
    "prompt = f'{subprompt} {entity} is best known for'\n",
    "outputs, inputs, start, end = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt=subprompt,\n",
    "    occurrence=0)\n",
    "reps = outputs.hidden_states[layer][0, start:end]\n",
    "reps = reps.detach().clone().requires_grad_(True)\n",
    "\n",
    "for name, parameter in model.named_parameters():\n",
    "    if 'transformer.h' not in name:\n",
    "        continue\n",
    "    l = int(name.split('.')[2])\n",
    "    if l < layer:\n",
    "        continue\n",
    "    parameter.requires_grad_(True)\n",
    "\n",
    "optimizer = optim.Adam((reps,), lr=lr)\n",
    "\n",
    "progress = tqdm(range(steps))\n",
    "for _ in progress:\n",
    "    outputs, *_ = run_model_with_reps(\n",
    "        entity,\n",
    "        prompt=subprompt,\n",
    "        layer=layer,\n",
    "        reps=reps,\n",
    "        labels=inputs.input_ids,\n",
    "    )\n",
    "    outputs.loss.backward()\n",
    "    progress.set_description(f'{outputs.loss.item():.3f}')\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c821e73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt=prompt,\n",
    "    generate=True,\n",
    "    max_length=60,\n",
    "    occurrence=0)\n",
    "print(tokenizer.batch_decode(outputs))\n",
    "outputs, *_ = run_model_with_reps(\n",
    "    entity,\n",
    "    prompt=prompt,\n",
    "    generate=True,\n",
    "    layer=layer,\n",
    "    reps=reps,\n",
    "    max_length=70,\n",
    "    occurrence=0)\n",
    "print(tokenizer.batch_decode(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae46360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
