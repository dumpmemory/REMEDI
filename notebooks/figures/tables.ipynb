{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RESULTS_ROOT = Path(\"../../results\")\n",
    "assert RESULTS_ROOT.exists()\n",
    "\n",
    "RESULTS_PREFIX = \"post_icml_\"  # Change to whatever you used\n",
    "MODEL = \"gptj\"\n",
    "\n",
    "OURMETHOD = r\"\\ourmethod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from experiments.aliases import REMEDI_EDITOR_LAYER, REMEDI_ENTITY_CLS_LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845561f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    with Path(file).open(\"r\") as handle:\n",
    "        data = json.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0e49f",
   "metadata": {},
   "source": [
    "# Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d875a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latexify_num(x):\n",
    "    return f\"${x}$\"\n",
    "\n",
    "def format_task(task):\n",
    "    accuracy = task[\"top1_accuracy\"]\n",
    "    fluency_mean = task[\"fluency\"][\"mean\"] * 100\n",
    "    fluency_std = task[\"fluency\"][\"std\"] * 100\n",
    "    return (\n",
    "        latexify_num(\n",
    "            f\"{accuracy:.2f}\".lstrip(\"0\"),\n",
    "        ),\n",
    "        latexify_num(f\"{fluency_mean:.1f}\")\n",
    "    )\n",
    "\n",
    "def latexify_row(row):\n",
    "    return \" & \".join(row) + r\" \\\\\"\n",
    "\n",
    "layer = REMEDI_EDITOR_LAYER[MODEL][\"biosbias\"]\n",
    "for method in (\"baseline\", OURMETHOD):\n",
    "    results_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_biosbias_{MODEL}\"\n",
    "    row = [method.capitalize()]\n",
    "    for task in (\"contextual\", \"decontextual\"):\n",
    "        if method == \"baseline\":\n",
    "            results_file = results_dir / task / \"baseline.json\"\n",
    "        else:\n",
    "            results_file = results_dir / task / f\"linear/{layer}/error_correction_metrics.json\"\n",
    "        assert results_file.exists()\n",
    "        results = load_json(results_file)\n",
    "        row += list(format_task(results.get(\"metrics\", results)))\n",
    "    row_str = latexify_row(row)\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a614e5",
   "metadata": {},
   "source": [
    "# Factual Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = REMEDI_EDITOR_LAYER[MODEL][\"counterfact\"]\n",
    "experiment_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_counterfact_{MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remedi import data, metrics\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load references from our own eval.\n",
    "references_file = experiment_dir / \"essence_references.json\"\n",
    "references = load_json(references_file)[\"references\"]\n",
    "references = [[r] for r in references if r]\n",
    "\n",
    "# Load the counterfact vectorizer.\n",
    "tfidf_vectorizer = data.load_counterfact_tfidf_vectorizer()\n",
    "\n",
    "# Load ROME essence results.\n",
    "essences_by_method = {}\n",
    "for method, results_dir in (\n",
    "    (\"FT\", \"../../../rome/results/FT-essence/run_000\"),\n",
    "    (\"ROME\", \"../../../rome/results/ROME-essence/run_000\"),\n",
    "):\n",
    "    case_files = sorted(Path(results_dir).glob(\"case*.json\"))\n",
    "\n",
    "    cases = []\n",
    "    for case_file in tqdm(case_files):\n",
    "        with case_file.open(\"r\") as handle:\n",
    "            case = json.load(handle)\n",
    "        cases.append(case)\n",
    "\n",
    "    cases = sorted(cases, key=lambda case: case[\"case_id\"])[:5000]\n",
    "\n",
    "    generations = []\n",
    "    for case in cases:\n",
    "        generations.append([case[\"post\"][\"generation\"]])\n",
    "\n",
    "    score = metrics.average_tfidf_similarity(generations, references, tfidf_vectorizer)\n",
    "    essences_by_method[method] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table rows for our method\n",
    "for method in (\"prefix\", \"replace\", OURMETHOD):\n",
    "    if method == OURMETHOD:\n",
    "        results_dir = experiment_dir / f\"linear/{layer}\"\n",
    "    else:\n",
    "        results_dir = experiment_dir / method\n",
    "    \n",
    "    scores = {}\n",
    "    for benchmark_name, keys in (\n",
    "        (\"efficacy\", (\"score\",)),\n",
    "        (\n",
    "            \"generation\",\n",
    "            (\n",
    "                \"fluency\",\n",
    "                \"consistency\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"essence\", (\"essence\",)),\n",
    "    ):\n",
    "        results_file = results_dir / f\"{benchmark_name}_metrics.json\"\n",
    "        with results_file.open(\"r\") as handle:\n",
    "            results = json.load(handle)\n",
    "\n",
    "        for key in keys:\n",
    "            scores[f\"{benchmark_name}_{key}\"] = results[key]\n",
    "\n",
    "    scores[\"neighborhood_score\"] = {\"mean\": 1.0, \"std\": 0.0}\n",
    "    \n",
    "    row = [method.capitalize()]\n",
    "    for key in (\n",
    "        \"efficacy_score\",\n",
    "        \"neighborhood_score\",\n",
    "        \"generation_consistency\",\n",
    "        \"generation_fluency\",\n",
    "        \"essence_essence\",\n",
    "    ):\n",
    "        score = scores[key][\"mean\"] * 100\n",
    "        row.append(latexify_num(f\"{score:.1f}\".lstrip(\"0\")))\n",
    "\n",
    "    print(latexify_row(row))\n",
    "\n",
    "print(r\"\\midrule\")\n",
    "print(r\"\\textbf{Model Edit} & & & & & \\\\\")\n",
    "print(r\"\\midrule\")\n",
    "\n",
    "# Print rows for ROME method.\n",
    "for method, results_dir in (\n",
    "    (\"FT\", \"../../../rome/results/FT/run_000\"),\n",
    "    (\"ROME\", \"../../../rome/results/ROME/run_000\"),\n",
    "):\n",
    "    summary_file = Path(results_dir) / \"summary.json\"\n",
    "    with summary_file.open(\"r\") as handle:\n",
    "        summary = json.load(handle)\n",
    "\n",
    "    row = [str(results_dir).split(\"/\")[-2]]\n",
    "    for metric in (\n",
    "        \"post_rewrite_success\",\n",
    "        \"post_neighborhood_success\",\n",
    "        \"post_reference_score\",\n",
    "        \"post_ngram_entropy\",\n",
    "        \"essence\",\n",
    "    ):\n",
    "        if metric == \"essence\":\n",
    "            x = essences_by_method[method]\n",
    "            mean = x.mean * 100\n",
    "        else:\n",
    "            mean, _ = summary[metric]\n",
    "        row.append(latexify_num(f\"{mean:.1f}\".lstrip(\"0\")))\n",
    "    print(latexify_row(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d329f61",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
