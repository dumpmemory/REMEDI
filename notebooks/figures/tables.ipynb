{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RESULTS_ROOT = Path(\"../../results\")\n",
    "assert RESULTS_ROOT.exists()\n",
    "\n",
    "RESULTS_PREFIX = \"emnlp_\"  # Change to whatever you used\n",
    "MODEL = \"gptj\"\n",
    "\n",
    "OURMETHOD = r\"\\ourmethod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from experiments.aliases import REMEDI_EDITOR_LAYER, REMEDI_ENTITY_CLS_LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845561f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    with Path(file).open(\"r\") as handle:\n",
    "        data = json.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0e49f",
   "metadata": {},
   "source": [
    "# Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d875a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latexify_num(x):\n",
    "    return f\"${x}$\"\n",
    "\n",
    "def format_task(task):\n",
    "    accuracy = task[\"top1_accuracy\"]\n",
    "    fluency_mean = task[\"fluency\"][\"mean\"] * 100\n",
    "    fluency_std = task[\"fluency\"][\"std\"] * 100\n",
    "    return (\n",
    "        latexify_num(\n",
    "            f\"{accuracy:.2f}\".lstrip(\"0\"),\n",
    "        ),\n",
    "        latexify_num(f\"{fluency_mean:.1f}\")\n",
    "    )\n",
    "\n",
    "def latexify_row(row):\n",
    "    return \" & \".join(row) + r\" \\\\\"\n",
    "\n",
    "layer = REMEDI_EDITOR_LAYER[MODEL][\"biosbias\"]\n",
    "for method in (\"baseline\", OURMETHOD):\n",
    "    results_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_biosbias_{MODEL}\"\n",
    "    row = [method.capitalize()]\n",
    "    for task in (\"contextual\", \"decontextual\"):\n",
    "        if method == \"baseline\":\n",
    "            results_file = results_dir / task / \"baseline.json\"\n",
    "        else:\n",
    "            results_file = results_dir / task / f\"linear/{layer}/error_correction_metrics.json\"\n",
    "        assert results_file.exists()\n",
    "        results = load_json(results_file)\n",
    "        row += list(format_task(results.get(\"metrics\", results)))\n",
    "    row_str = latexify_row(row)\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a614e5",
   "metadata": {},
   "source": [
    "# Factual Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = REMEDI_EDITOR_LAYER[MODEL][\"counterfact\"]\n",
    "experiment_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_counterfact_{MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remedi import data, metrics\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load references from our own eval.\n",
    "references_file = experiment_dir / \"essence_references.json\"\n",
    "references = load_json(references_file)[\"references\"]\n",
    "references = [[r] for r in references if r]\n",
    "\n",
    "# Load the counterfact vectorizer.\n",
    "tfidf_vectorizer = data.load_counterfact_tfidf_vectorizer()\n",
    "\n",
    "# Load ROME essence results.\n",
    "essences_by_method = {}\n",
    "for method, results_dir in (\n",
    "    (\"FT\", \"../../../rome/results/FT-essence/run_000\"),\n",
    "    (\"ROME\", \"../../../rome/results/ROME-essence/run_000\"),\n",
    "):\n",
    "    case_files = sorted(Path(results_dir).glob(\"case*.json\"))\n",
    "\n",
    "    cases = []\n",
    "    for case_file in tqdm(case_files):\n",
    "        with case_file.open(\"r\") as handle:\n",
    "            case = json.load(handle)\n",
    "        cases.append(case)\n",
    "\n",
    "    cases = sorted(cases, key=lambda case: case[\"case_id\"])[:5000]\n",
    "\n",
    "    generations = []\n",
    "    for case in cases:\n",
    "        generations.append([case[\"post\"][\"generation\"]])\n",
    "\n",
    "    score = metrics.average_tfidf_similarity(generations, references, tfidf_vectorizer)\n",
    "    essences_by_method[method] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table rows for our method\n",
    "for method in (\"prefix\", \"replace\", OURMETHOD):\n",
    "    if method == OURMETHOD:\n",
    "        results_dir = experiment_dir / f\"linear/{layer}\"\n",
    "    else:\n",
    "        results_dir = experiment_dir / method\n",
    "\n",
    "    scores = {}\n",
    "    for benchmark_name, keys in (\n",
    "        (\"efficacy\", (\"score\",)),\n",
    "        (\n",
    "            \"generation\",\n",
    "            (\n",
    "                \"fluency\",\n",
    "                \"consistency\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"essence\", (\"essence\",)),\n",
    "    ):\n",
    "        results_file = results_dir / f\"{benchmark_name}_metrics.json\"\n",
    "        with results_file.open(\"r\") as handle:\n",
    "            results = json.load(handle)\n",
    "\n",
    "        for key in keys:\n",
    "            scores[f\"{benchmark_name}_{key}\"] = results[key]\n",
    "\n",
    "    scores[\"neighborhood_score\"] = {\"mean\": 1.0, \"std\": 0.0}\n",
    "\n",
    "    row = [method.capitalize()]\n",
    "    for key in (\n",
    "        \"efficacy_score\",\n",
    "        \"neighborhood_score\",\n",
    "        \"generation_consistency\",\n",
    "        \"generation_fluency\",\n",
    "        \"essence_essence\",\n",
    "    ):\n",
    "        score = scores[key][\"mean\"] * 100\n",
    "        row.append(latexify_num(f\"{score:.1f}\".lstrip(\"0\")))\n",
    "\n",
    "    print(latexify_row(row))\n",
    "\n",
    "print(r\"\\midrule\")\n",
    "print(r\"\\textbf{Model Edit} & & & & & \\\\\")\n",
    "print(r\"\\midrule\")\n",
    "\n",
    "# Print rows for ROME method.\n",
    "for method, results_dir in (\n",
    "    (\"FT\", \"../../../rome/results/FT/run_000\"),\n",
    "    (\"ROME\", \"../../../rome/results/ROME/run_000\"),\n",
    "):\n",
    "    summary_file = Path(results_dir) / \"summary.json\"\n",
    "    with summary_file.open(\"r\") as handle:\n",
    "        summary = json.load(handle)\n",
    "\n",
    "    row = [str(results_dir).split(\"/\")[-2]]\n",
    "    for metric in (\n",
    "        \"post_rewrite_success\",\n",
    "        \"post_neighborhood_success\",\n",
    "        \"post_reference_score\",\n",
    "        \"post_ngram_entropy\",\n",
    "        \"essence\",\n",
    "    ):\n",
    "        if metric == \"essence\":\n",
    "            x = essences_by_method[method]\n",
    "            mean = x.mean * 100\n",
    "        else:\n",
    "            mean, _ = summary[metric]\n",
    "        row.append(latexify_num(f\"{mean:.1f}\".lstrip(\"0\")))\n",
    "    print(latexify_row(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e175ad94",
   "metadata": {},
   "source": [
    "For the appendix, break the scores down by:\n",
    "- Model knows vs. model does not know.\n",
    "- Model saw relation during training vs. model did not\n",
    "- Relation type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcdf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "remedi_layer = REMEDI_EDITOR_LAYER[MODEL][\"counterfact\"]\n",
    "entity_layer = REMEDI_ENTITY_CLS_LAYER[MODEL][\"counterfact\"]\n",
    "gen_experiment_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_counterfact_{MODEL}\"\n",
    "cls_experiment_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_cls_counterfact_{MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500eb164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from remedi import data, metrics\n",
    "\n",
    "\n",
    "def normalize_attribute(entity, context):\n",
    "    return context.replace(entity, \"[entity]\")\n",
    "\n",
    "\n",
    "counterfact_train = data.load_dataset(\"counterfact\", split=\"train[:5000]\")\n",
    "counterfact_test = data.load_dataset(\"counterfact\", split=\"train[5000:10000]\")\n",
    "attributes_train = {normalize_attribute(x[\"entity\"], x[\"context\"]) for x in counterfact_train}\n",
    "\n",
    "attributes_by_relation = defaultdict(set)\n",
    "for x in chain(counterfact_train, counterfact_test):\n",
    "    rid = x[\"source\"][\"requested_rewrite\"][\"relation_id\"]\n",
    "    attribute = normalize_attribute(x[\"entity\"], x[\"context\"]).replace(x[\"target_mediated\"], \"[target]\")\n",
    "    attributes_by_relation[rid].add(attribute)\n",
    "relation_canonical = {\n",
    "    rid: next(iter(sorted(\n",
    "        [a for a in attrs if \"?\" not in a],\n",
    "        key=len,\n",
    "        reverse=True,\n",
    "    )))\n",
    "    for rid, attrs in attributes_by_relation.items()\n",
    "}\n",
    "\n",
    "\n",
    "def group_by_model_knows(x_dataset, x_cls):\n",
    "    return \"Model Knows\" if x_cls.label else \"Model Does Not Know\"\n",
    "\n",
    "\n",
    "def group_by_generalization(x_dataset, x_cls):\n",
    "    attribute = normalize_attribute(x_dataset[\"entity\"], x_dataset[\"context\"])\n",
    "    return \"Seen in Training\" if attribute in attributes_train else \"Unseen in Training\"\n",
    "\n",
    "\n",
    "def group_by_relation_type(x_dataset, x_cls):\n",
    "    return relation_canonical[x_dataset[\"source\"][\"requested_rewrite\"][\"relation_id\"]]\n",
    "\n",
    "\n",
    "gen_results_dir = gen_experiment_dir / f\"linear/{layer}\"\n",
    "cls_results = load_json(\n",
    "    cls_experiment_dir\n",
    "    / \"linear\"\n",
    "    / str(remedi_layer)\n",
    "    / f\"fact_cls_layer_{entity_layer}.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48321b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remedi import benchmarks\n",
    "\n",
    "gen_scores = defaultdict(list)\n",
    "for benchmark_name in (\n",
    "    \"efficacy\",\n",
    "    \"generation\",\n",
    "    \"essence\",\n",
    "):\n",
    "    gen_results_file = gen_results_dir / f\"{benchmark_name}.json\"\n",
    "    with gen_results_file.open(\"r\") as handle:\n",
    "        results = json.load(handle)\n",
    "\n",
    "    for x in results[\"samples\"]:\n",
    "        if benchmark_name == \"efficacy\":\n",
    "            gen_scores[\"efficacy\"].append(x[\"target_score\"] > x[\"comparator_score\"])\n",
    "        elif benchmark_name == \"generation\":\n",
    "            gen_scores[\"fluency\"].append(x[\"fluency_score\"])\n",
    "            gen_scores[\"consistency\"].append(x[\"consistency_score\"])\n",
    "        else:\n",
    "            assert benchmark_name == \"essence\"\n",
    "            gen_scores[\"essence\"].append(x[\"essence_score\"])\n",
    "\n",
    "\n",
    "def make_table(group_by):\n",
    "    idx_by_group = defaultdict(list)\n",
    "    for i, (x_train, x_cls) in enumerate(zip(counterfact_test, cls_results[\"samples\"])):\n",
    "        group = group_by(x_train, benchmarks.ClassificationSample.from_dict(x_cls).decontextual)\n",
    "        idx_by_group[group].append(i)\n",
    "\n",
    "    for group, idx in idx_by_group.items():\n",
    "        group_scores = {\n",
    "            key: metrics.Metric.aggregate([values[i] for i in idx]).mean\n",
    "            for key, values in gen_scores.items()\n",
    "        }\n",
    "        group_scores[\"total\"] = len(idx)\n",
    "\n",
    "        row = [group]\n",
    "        for key in (\n",
    "            \"total\",\n",
    "            \"efficacy\",\n",
    "            \"fluency\",\n",
    "            \"consistency\",\n",
    "            \"essence\",\n",
    "        ):\n",
    "            score = group_scores[key]\n",
    "            if key != \"total\":\n",
    "                score *= 100\n",
    "                score_str = latexify_num(f\"{score:.1f}\".lstrip(\"0\"))\n",
    "            else:\n",
    "                score_str = str(int(score))\n",
    "            row.append(score_str)\n",
    "\n",
    "        print(latexify_row(row))\n",
    "\n",
    "make_table(group_by_model_knows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(group_by_generalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca04bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_table(group_by_relation_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d329f61",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "METHOD_REMEDI = r\"\\ourmethod\"\n",
    "METHOD_REMEDI_I = r\"\\ourmethod ($I$)\"\n",
    "METHOD_CONTROL_TASK = \"Task\"\n",
    "METHOD_CONTROL_MODEL = \"Model\"\n",
    "METHODS = (METHOD_REMEDI, METHOD_REMEDI_I, METHOD_CONTROL_TASK, METHOD_CONTROL_MODEL)\n",
    "\n",
    "TASK_BIOS_MED = \"Bios-Med\"\n",
    "TASK_FACT_MED = \"Fact-Med\"\n",
    "TASK_FACT_PRIOR = \"Fact-Prior\"\n",
    "\n",
    "fact_remedi_layer = REMEDI_EDITOR_LAYER[MODEL][\"counterfact\"]\n",
    "fact_entity_layer = REMEDI_ENTITY_CLS_LAYER[MODEL][\"counterfact\"]\n",
    "bios_remedi_layer = REMEDI_EDITOR_LAYER[MODEL][\"biosbias\"]\n",
    "bios_entity_layer = REMEDI_ENTITY_CLS_LAYER[MODEL][\"biosbias\"]\n",
    "\n",
    "bios_results_dir = RESULTS_ROOT / f\"post_icml_eval_cls_biosbias_{MODEL}\"\n",
    "fact_results_dir = RESULTS_ROOT / f\"post_icml_eval_cls_counterfact_{MODEL}\"\n",
    "for results_dir in (bios_results_dir, fact_results_dir):\n",
    "    assert results_dir.exists()\n",
    "\n",
    "\n",
    "results_by_method = defaultdict(dict)\n",
    "for method in METHODS:\n",
    "    if method == METHOD_REMEDI:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "            bios_results_dir\n",
    "            / \"linear\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_metrics.json\"\n",
    "        )\n",
    "        \n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"linear\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    elif method == METHOD_REMEDI_I:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "            bios_results_dir\n",
    "            / \"identity\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_metrics.json\"\n",
    "        )\n",
    "\n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"identity\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    elif method == METHOD_CONTROL_TASK:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "        bios_results_dir\n",
    "            / \"linear\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_control_task_metrics.json\"\n",
    "        )\n",
    "\n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"linear\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_control_task_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    elif method == METHOD_CONTROL_MODEL:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "        bios_results_dir\n",
    "            / \"linear\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_control_model_metrics.json\"\n",
    "        )\n",
    "\n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"linear\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_control_model_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    else:\n",
    "        assert False, method\n",
    "\n",
    "rows = []\n",
    "for method in METHODS:\n",
    "    if method == METHOD_CONTROL_TASK:\n",
    "        rows += [[r\"\\midrule\"], [r\"\\textbf{Control}\", *([\"\"] * 6)], [r\"\\midrule\"]]\n",
    "    row = [method]\n",
    "    for task, score in (\n",
    "        (TASK_BIOS_MED, \"f1\"),\n",
    "        (TASK_BIOS_MED, \"mcc\"),\n",
    "        (TASK_FACT_MED, \"f1\"),\n",
    "        (TASK_FACT_MED, \"mcc\"),\n",
    "        (TASK_FACT_PRIOR, \"f1\"),\n",
    "        (TASK_FACT_PRIOR, \"mcc\"),\n",
    "    ):\n",
    "        row.append(results_by_method[method][task][score])\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42047763",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = \"\\n\"\n",
    "table = sep.join(\n",
    "    \" & \".join(\n",
    "        \"$\" + f\"{x:.2f}\".lstrip(\"0\") + \"$\" if isinstance(x, float) else x\n",
    "        for x in row\n",
    "    ) + (r\" \\\\\" if len(row) > 1 else \"\")\n",
    "    for row in rows\n",
    ") + sep\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4a207",
   "metadata": {},
   "source": [
    "# Entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "remedi_layer = REMEDI_EDITOR_LAYER[MODEL][\"mcrae\"]\n",
    "experiment_name = f\"post_icml_eval_ent_mcrae_{MODEL}\"\n",
    "results_dir = RESULTS_ROOT / experiment_name\n",
    "\n",
    "prefix_metrics = load_json(results_dir / \"prefix/entailment_metrics.json\")\n",
    "remedi_metrics = load_json(results_dir / \"linear\" / str(remedi_layer) / \"entailment_metrics.json\")\n",
    "\n",
    "prefix_results = load_json(results_dir / \"prefix/entailment.json\")\n",
    "remedi_results = load_json(results_dir / \"linear\" / str(remedi_layer) / \"entailment.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498eddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remedi import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_logp_delta_remedi(key):\n",
    "    deltas = []\n",
    "    deltas_pct = []\n",
    "    for x in remedi_results[\"samples\"]:\n",
    "        features = x[f\"{key}_features\"]\n",
    "\n",
    "        x_ds = []\n",
    "        x_ds_pct = []\n",
    "        for f in features:\n",
    "            logp_post = f[\"logp_post\"]\n",
    "            logp_pre = f[\"logp_pre\"]\n",
    "            x_ds.append(np.exp(logp_post) - np.exp(logp_pre))\n",
    "            x_ds_pct.append((logp_post - logp_pre) / abs(logp_pre) * 100)\n",
    "        deltas.append(np.mean(x_ds))\n",
    "        deltas_pct.append(np.mean(x_ds_pct))\n",
    "    return {\n",
    "\n",
    "        f\"{key}_p_delta\": metrics.Metric.aggregate(deltas).to_dict(),\n",
    "        f\"{key}_p_delta_pct\": metrics.Metric.aggregate(deltas_pct).to_dict(),\n",
    "    }\n",
    "\n",
    "\n",
    "def get_logp_delta_prefix(key):\n",
    "    deltas = []\n",
    "    deltas_pct = []\n",
    "    for x_rem, x_pre in zip(remedi_results[\"samples\"], prefix_results[\"samples\"]):\n",
    "        features_rem = x_rem[f\"{key}_features\"]\n",
    "        features_pre = x_pre[f\"{key}_features\"]\n",
    "\n",
    "        x_ds = []\n",
    "        x_ds_pct = []\n",
    "        for f_rem, f_pre in zip(features_rem, features_pre):\n",
    "            logp_pre = f_rem[\"logp_pre\"]\n",
    "            logp_post = f_pre[\"logp_pre\"]\n",
    "            x_ds.append(np.exp(logp_post) - np.exp(logp_pre))\n",
    "            x_ds_pct.append((logp_post - logp_pre) / abs(logp_pre) * 100)\n",
    "\n",
    "        deltas.append(np.mean(x_ds))\n",
    "        deltas_pct.append(np.mean(x_ds_pct))\n",
    "    return {\n",
    "        f\"{key}_p_delta\": metrics.Metric.aggregate(deltas).to_dict(),\n",
    "        f\"{key}_p_delta_pct\": metrics.Metric.aggregate(deltas_pct).to_dict(),\n",
    "    }\n",
    "\n",
    "# Need to compute probability deltas.\n",
    "remedi_metrics.update({\n",
    "    **get_logp_delta_remedi(\"co\"),\n",
    "    **get_logp_delta_remedi(\"orig\"),\n",
    "    **get_logp_delta_remedi(\"unrel\"),\n",
    "})\n",
    "prefix_metrics.update({\n",
    "    **get_logp_delta_prefix(\"co\"),\n",
    "    **get_logp_delta_prefix(\"orig\"),\n",
    "    **get_logp_delta_prefix(\"unrel\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1219b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "remedi_metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f16d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  When   | Original Correlation | Entailed Correlation\n",
    "#  Before | * *\n",
    "#  Prefix | * *\n",
    "#  REMEDI | * *\n",
    "def format_p_delta(key, metrics):\n",
    "    delta = metrics[f\"{key}_p_delta\"]\n",
    "    mean = delta[\"mean\"]\n",
    "    if abs(mean) < .0001:\n",
    "        return \"0\"\n",
    "    mean = mean * 100\n",
    "    std = delta[\"std\"] * 100\n",
    "    return f\"${mean:.1f}$ $({std:.1f})$\"\n",
    "\n",
    "rows = [\n",
    "    [\n",
    "        \"No Edit\",\n",
    "        \"--\",\n",
    "        remedi_metrics[\"co_corr_pre\"],\n",
    "        \"--\",\n",
    "        remedi_metrics[\"orig_corr_pre\"],\n",
    "        \"--\",\n",
    "    ],\n",
    "    [\n",
    "        \"Prefix\",\n",
    "        format_p_delta(\"co\", prefix_metrics),\n",
    "        prefix_metrics[\"co_corr_pre\"],\n",
    "        format_p_delta(\"orig\", prefix_metrics),\n",
    "        prefix_metrics[\"orig_corr_pre\"],\n",
    "        format_p_delta(\"unrel\", prefix_metrics),\n",
    "    ],\n",
    "    [\n",
    "        r\"\\ourmethod\",\n",
    "        format_p_delta(\"co\", remedi_metrics),\n",
    "        remedi_metrics[\"co_corr_post\"],\n",
    "        format_p_delta(\"orig\", remedi_metrics),\n",
    "        remedi_metrics[\"orig_corr_post\"],\n",
    "        format_p_delta(\"unrel\", remedi_metrics),\n",
    "    ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76443da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = r\" \\\\\" + \"\\n\"\n",
    "table = sep.join(\n",
    "    \" & \".join(\"$\" + f\"{x['mean']:.2f}$\".lstrip('0') if isinstance(x, dict) else x for x in row)\n",
    "    for row in rows\n",
    ") + sep\n",
    "print(table.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981dac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{43.1:.0f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "remedi_metrics[\"orig_delta\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2e9802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
