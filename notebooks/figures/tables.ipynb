{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "RESULTS_ROOT = Path(\"../../results\")\n",
    "assert RESULTS_ROOT.exists()\n",
    "\n",
    "RESULTS_PREFIX = \"post_icml_\"  # Change to whatever you used\n",
    "MODEL = \"gptj\"\n",
    "\n",
    "OURMETHOD = r\"\\ourmethod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6129cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from experiments.aliases import REMEDI_EDITOR_LAYER, REMEDI_ENTITY_CLS_LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845561f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_json(file):\n",
    "    with Path(file).open(\"r\") as handle:\n",
    "        data = json.load(handle)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff0e49f",
   "metadata": {},
   "source": [
    "# Error Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d875a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latexify_num(x):\n",
    "    return f\"${x}$\"\n",
    "\n",
    "def format_task(task):\n",
    "    accuracy = task[\"top1_accuracy\"]\n",
    "    fluency_mean = task[\"fluency\"][\"mean\"] * 100\n",
    "    fluency_std = task[\"fluency\"][\"std\"] * 100\n",
    "    return (\n",
    "        latexify_num(\n",
    "            f\"{accuracy:.2f}\".lstrip(\"0\"),\n",
    "        ),\n",
    "        latexify_num(f\"{fluency_mean:.1f}\")\n",
    "    )\n",
    "\n",
    "def latexify_row(row):\n",
    "    return \" & \".join(row) + r\" \\\\\"\n",
    "\n",
    "layer = REMEDI_EDITOR_LAYER[MODEL][\"biosbias\"]\n",
    "for method in (\"baseline\", OURMETHOD):\n",
    "    results_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_biosbias_{MODEL}\"\n",
    "    row = [method.capitalize()]\n",
    "    for task in (\"contextual\", \"decontextual\"):\n",
    "        if method == \"baseline\":\n",
    "            results_file = results_dir / task / \"baseline.json\"\n",
    "        else:\n",
    "            results_file = results_dir / task / f\"linear/{layer}/error_correction_metrics.json\"\n",
    "        assert results_file.exists()\n",
    "        results = load_json(results_file)\n",
    "        row += list(format_task(results.get(\"metrics\", results)))\n",
    "    row_str = latexify_row(row)\n",
    "    print(row_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a614e5",
   "metadata": {},
   "source": [
    "# Factual Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b2234",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = REMEDI_EDITOR_LAYER[MODEL][\"counterfact\"]\n",
    "experiment_dir = RESULTS_ROOT / f\"{RESULTS_PREFIX}eval_gen_counterfact_{MODEL}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96ab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remedi import data, metrics\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load references from our own eval.\n",
    "references_file = experiment_dir / \"essence_references.json\"\n",
    "references = load_json(references_file)[\"references\"]\n",
    "references = [[r] for r in references if r]\n",
    "\n",
    "# Load the counterfact vectorizer.\n",
    "tfidf_vectorizer = data.load_counterfact_tfidf_vectorizer()\n",
    "\n",
    "# Load ROME essence results.\n",
    "essences_by_method = {}\n",
    "for method, results_dir in (\n",
    "    (\"FT\", \"../../../rome/results/FT-essence/run_000\"),\n",
    "    (\"ROME\", \"../../../rome/results/ROME-essence/run_000\"),\n",
    "):\n",
    "    case_files = sorted(Path(results_dir).glob(\"case*.json\"))\n",
    "\n",
    "    cases = []\n",
    "    for case_file in tqdm(case_files):\n",
    "        with case_file.open(\"r\") as handle:\n",
    "            case = json.load(handle)\n",
    "        cases.append(case)\n",
    "\n",
    "    cases = sorted(cases, key=lambda case: case[\"case_id\"])[:5000]\n",
    "\n",
    "    generations = []\n",
    "    for case in cases:\n",
    "        generations.append([case[\"post\"][\"generation\"]])\n",
    "\n",
    "    score = metrics.average_tfidf_similarity(generations, references, tfidf_vectorizer)\n",
    "    essences_by_method[method] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724a526b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table rows for our method\n",
    "for method in (\"prefix\", \"replace\", OURMETHOD):\n",
    "    if method == OURMETHOD:\n",
    "        results_dir = experiment_dir / f\"linear/{layer}\"\n",
    "    else:\n",
    "        results_dir = experiment_dir / method\n",
    "    \n",
    "    scores = {}\n",
    "    for benchmark_name, keys in (\n",
    "        (\"efficacy\", (\"score\",)),\n",
    "        (\n",
    "            \"generation\",\n",
    "            (\n",
    "                \"fluency\",\n",
    "                \"consistency\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"essence\", (\"essence\",)),\n",
    "    ):\n",
    "        results_file = results_dir / f\"{benchmark_name}_metrics.json\"\n",
    "        with results_file.open(\"r\") as handle:\n",
    "            results = json.load(handle)\n",
    "\n",
    "        for key in keys:\n",
    "            scores[f\"{benchmark_name}_{key}\"] = results[key]\n",
    "\n",
    "    scores[\"neighborhood_score\"] = {\"mean\": 1.0, \"std\": 0.0}\n",
    "    \n",
    "    row = [method.capitalize()]\n",
    "    for key in (\n",
    "        \"efficacy_score\",\n",
    "        \"neighborhood_score\",\n",
    "        \"generation_consistency\",\n",
    "        \"generation_fluency\",\n",
    "        \"essence_essence\",\n",
    "    ):\n",
    "        score = scores[key][\"mean\"] * 100\n",
    "        row.append(latexify_num(f\"{score:.1f}\".lstrip(\"0\")))\n",
    "\n",
    "    print(latexify_row(row))\n",
    "\n",
    "print(r\"\\midrule\")\n",
    "print(r\"\\textbf{Model Edit} & & & & & \\\\\")\n",
    "print(r\"\\midrule\")\n",
    "\n",
    "# Print rows for ROME method.\n",
    "for method, results_dir in (\n",
    "    (\"FT\", \"../../../rome/results/FT/run_000\"),\n",
    "    (\"ROME\", \"../../../rome/results/ROME/run_000\"),\n",
    "):\n",
    "    summary_file = Path(results_dir) / \"summary.json\"\n",
    "    with summary_file.open(\"r\") as handle:\n",
    "        summary = json.load(handle)\n",
    "\n",
    "    row = [str(results_dir).split(\"/\")[-2]]\n",
    "    for metric in (\n",
    "        \"post_rewrite_success\",\n",
    "        \"post_neighborhood_success\",\n",
    "        \"post_reference_score\",\n",
    "        \"post_ngram_entropy\",\n",
    "        \"essence\",\n",
    "    ):\n",
    "        if metric == \"essence\":\n",
    "            x = essences_by_method[method]\n",
    "            mean = x.mean * 100\n",
    "        else:\n",
    "            mean, _ = summary[metric]\n",
    "        row.append(latexify_num(f\"{mean:.1f}\".lstrip(\"0\")))\n",
    "    print(latexify_row(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d329f61",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de0e72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "METHOD_REMEDI = r\"\\ourmethod\"\n",
    "METHOD_REMEDI_I = r\"\\ourmethod ($I$)\"\n",
    "METHOD_CONTROL_TASK = \"Task\"\n",
    "METHOD_CONTROL_MODEL = \"Model\"\n",
    "METHODS = (METHOD_REMEDI, METHOD_REMEDI_I, METHOD_CONTROL_TASK, METHOD_CONTROL_MODEL)\n",
    "\n",
    "TASK_BIOS_MED = \"Bios-Med\"\n",
    "TASK_FACT_MED = \"Fact-Med\"\n",
    "TASK_FACT_PRIOR = \"Fact-Prior\"\n",
    "\n",
    "fact_remedi_layer = REMEDI_EDITOR_LAYER[MODEL][\"counterfact\"]\n",
    "fact_entity_layer = REMEDI_ENTITY_CLS_LAYER[MODEL][\"counterfact\"]\n",
    "bios_remedi_layer = REMEDI_EDITOR_LAYER[MODEL][\"biosbias\"]\n",
    "bios_entity_layer = REMEDI_ENTITY_CLS_LAYER[MODEL][\"biosbias\"]\n",
    "\n",
    "bios_results_dir = RESULTS_ROOT / f\"post_icml_eval_cls_biosbias_{MODEL}\"\n",
    "fact_results_dir = RESULTS_ROOT / f\"post_icml_eval_cls_counterfact_{MODEL}\"\n",
    "for results_dir in (bios_results_dir, fact_results_dir):\n",
    "    assert results_dir.exists()\n",
    "\n",
    "\n",
    "results_by_method = defaultdict(dict)\n",
    "for method in METHODS:\n",
    "    if method == METHOD_REMEDI:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "            bios_results_dir\n",
    "            / \"linear\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_metrics.json\"\n",
    "        )\n",
    "        \n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"linear\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    elif method == METHOD_REMEDI_I:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "            bios_results_dir\n",
    "            / \"identity\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_metrics.json\"\n",
    "        )\n",
    "\n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"identity\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    elif method == METHOD_CONTROL_TASK:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "        bios_results_dir\n",
    "            / \"linear\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_control_task_metrics.json\"\n",
    "        )\n",
    "\n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"linear\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_control_task_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    elif method == METHOD_CONTROL_MODEL:\n",
    "        results_by_method[method][TASK_BIOS_MED] = load_json(\n",
    "        bios_results_dir\n",
    "            / \"linear\"\n",
    "            / str(bios_remedi_layer)\n",
    "            / f\"error_cls_layer_{bios_entity_layer}_control_model_metrics.json\"\n",
    "        )\n",
    "\n",
    "        fact_results = load_json(\n",
    "            fact_results_dir\n",
    "            / \"linear\"\n",
    "            / str(fact_remedi_layer)\n",
    "            / f\"fact_cls_layer_{fact_entity_layer}_control_model_metrics.json\"\n",
    "        )\n",
    "        results_by_method[method][TASK_FACT_MED] = fact_results[\"contextual\"]\n",
    "        results_by_method[method][TASK_FACT_PRIOR] = fact_results[\"decontextual\"]\n",
    "    else:\n",
    "        assert False, method\n",
    "\n",
    "rows = []\n",
    "for method in METHODS:\n",
    "    if method == METHOD_CONTROL_TASK:\n",
    "        rows += [[r\"\\midrule\"], [\"Control\", *([\"&\"] * 6)], [r\"\\midrule\"]]\n",
    "    row = [method]\n",
    "    for task, score in (\n",
    "        (TASK_BIOS_MED, \"f1\"),\n",
    "        (TASK_BIOS_MED, \"mcc\"),\n",
    "        (TASK_FACT_MED, \"f1\"),\n",
    "        (TASK_FACT_MED, \"mcc\"),\n",
    "        (TASK_FACT_PRIOR, \"f1\"),\n",
    "        (TASK_FACT_PRIOR, \"mcc\"),\n",
    "    ):\n",
    "        row.append(results_by_method[method][task][score])\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4842f915",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = r\" \\\\\" + \"\\n\"\n",
    "table = sep.join(\n",
    "    \" & \".join(\n",
    "        f\"{x:.2f}\".lstrip('0') if isinstance(x, float) else x\n",
    "        for x in row[1:]\n",
    "    )\n",
    "    for row in rows\n",
    ")\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f547b4c8",
   "metadata": {},
   "source": [
    "# [Delete] Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4b7a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from remedi import benchmarks\n",
    "\n",
    "fact_results = benchmarks.ClassificationBenchmarkResults.from_dict(load_json(\n",
    "    fact_results_dir\n",
    "    / \"linear\"\n",
    "    / str(fact_remedi_layer)\n",
    "    / f\"fact_cls_layer_{fact_entity_layer}.json\"\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b95d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(x.decontextual.prediction for x in fact_results.samples) / len(fact_results.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a042e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, matthews_corrcoef\n",
    "\n",
    "import random\n",
    "labels = [not x.decontextual.label for x in fact_results.samples]\n",
    "predictions = [not x.decontextual.prediction for x in fact_results.samples]\n",
    "\n",
    "random.shuffle(labels)\n",
    "\n",
    "print(accuracy_score(labels, predictions))\n",
    "print(f1_score(labels, predictions))\n",
    "print(matthews_corrcoef(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec213938",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
