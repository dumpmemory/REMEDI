{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5eb05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/raid/lingo/dez/code/context-mediation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d16ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "device = \"cuda\"\n",
    "config = \"gpt2-xl\"\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(config).to(device).eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(config)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e3a56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from src.utils import tokenizers\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def load_and_preprocess(file, root=\"/raid/lingo/dez/code/rome/data/mediation\"):\n",
    "    with Path(root, file).open(\"r\") as handle:\n",
    "        dataset = json.load(handle)\n",
    "    is_counterfact = \"counterfact\" in file\n",
    "    for sample in tqdm(dataset, desc=f\"load and preprocess {file}\"):\n",
    "        subject = sample[\"subject\"]\n",
    "        prompt = sample[\"prompt\"]\n",
    "        mediated = sample[\"attribute\"]\n",
    "        unmediated = sample[\"comparator\"]\n",
    "        sample[\"prompt_mediated\"] = prompt_mediated = f\"{prompt} {mediated}\"\n",
    "        sample[\"prompt_unmediated\"] = prompt_unmediated = f\"{prompt} {unmediated}\"\n",
    "        sample[\"token_range_mediated_attr\"] = tokenizers.find_token_range(\n",
    "            prompt_mediated,\n",
    "            mediated,\n",
    "            tokenizer,\n",
    "            occurrence=1 if is_counterfact else 0,\n",
    "        )\n",
    "        sample[\"token_range_unmediated_attr\"] = tokenizers.find_token_range(\n",
    "            prompt_unmediated,\n",
    "            unmediated,\n",
    "            tokenizer,\n",
    "        )\n",
    "        sample[\"token_range_subject_first\"] = tokenizers.find_token_range(\n",
    "            prompt if is_counterfact else prompt.lower(),\n",
    "            subject if is_counterfact else subject.lower(),\n",
    "            tokenizer,\n",
    "            occurrence=0)\n",
    "        sample[\"token_range_subject_last\"] = tokenizers.find_token_range(\n",
    "            prompt if is_counterfact else prompt.lower(),\n",
    "            subject if is_counterfact else subject.lower(),\n",
    "            tokenizer,\n",
    "            occurrence=sample[\"occurrence\"])\n",
    "    return dataset\n",
    "\n",
    "winoventi = load_and_preprocess(\"winoventi_subj_last.json\")\n",
    "counterfact = load_and_preprocess(\"counterfact_med_subj_last.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e5cf9",
   "metadata": {},
   "source": [
    "# Evaluate Mediation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "def compute_logprobs(inputs, outputs, ranges):\n",
    "    seq_token_logprobs = torch.log_softmax(outputs.logits, dim=-1)\n",
    "    logprobs = []\n",
    "    for tokens, token_logprobs, start, end in zip(inputs.input_ids,\n",
    "                                                  seq_token_logprobs,\n",
    "                                                  *ranges):\n",
    "        logprob = token_logprobs[torch.arange(start, end), tokens[start:end]].sum()\n",
    "        logprobs.append(logprob)\n",
    "    return torch.tensor(logprobs)\n",
    " \n",
    "def compute_gt_indices(logp_left, logp_right):\n",
    "    return logp_left\\\n",
    "        .gt(logp_right)\\\n",
    "        .nonzero()\\\n",
    "        .squeeze()\\\n",
    "        .tolist() \n",
    "\n",
    "def evaluate(dataset, batch_size=64):\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "    meds, umeds = [], []\n",
    "    with torch.inference_mode():\n",
    "        for bi, batch in enumerate(tqdm(loader)):\n",
    "            logprobs = {}\n",
    "            for key in (\"mediated\", \"unmediated\"):\n",
    "                texts = batch[f\"prompt_{key}\"]\n",
    "                ranges = batch[f\"token_range_{key}_attr\"]\n",
    "                inputs = tokenizer(\n",
    "                    list(texts),\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=\"longest\").to(device)\n",
    "                outputs = model(**inputs)\n",
    "                logprobs[key] = compute_logprobs(inputs, outputs, ranges)\n",
    "            meds_idx = compute_gt_indices(logprobs[\"mediated\"],\n",
    "                                          logprobs[\"unmediated\"])\n",
    "            umeds_idx = compute_gt_indices(logprobs[\"unmediated\"],\n",
    "                                           logprobs[\"mediated\"])\n",
    "            offset = bi * batch_size\n",
    "            for indices, results in ((meds_idx, meds), (umeds_idx, umeds)):\n",
    "                results += [\n",
    "                    dict(\n",
    "                        logp_mediated=torch.exp(logprobs[\"mediated\"][index]).item(),\n",
    "                        logp_unmediated=torch.exp(logprobs[\"unmediated\"][index]).item(),\n",
    "                        **dataset[offset + index],\n",
    "                    )\n",
    "                    for index in indices\n",
    "                ]\n",
    "\n",
    "    return len(meds) / (len(meds) + len(umeds)), meds, umeds\n",
    "\n",
    "counterfact_results = evaluate(counterfact)\n",
    "print(\"CF:\", counterfact_results[0])\n",
    "\n",
    "winoventi_results = evaluate(winoventi)\n",
    "print(\"WV:\", winoventi_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a619db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(winoventi_results[2][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff3d66",
   "metadata": {},
   "source": [
    "# Fix Mediation\n",
    "\n",
    "Precompute inputs and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c016a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e2deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER = 15\n",
    "LR=5e-5\n",
    "MAX_EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 10\n",
    "DATASET = counterfact_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bcc227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nethook\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def precompute_hiddens(model,\n",
    "                       tokenizer,\n",
    "                       samples,\n",
    "                       device=device,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       layer=LAYER,\n",
    "                       desc=\"precompute hiddens\"):\n",
    "    model = model.eval().to(device)\n",
    "    loader = torch.utils.data.DataLoader(samples, batch_size=batch_size)\n",
    "    hiddens = []\n",
    "    for batch in tqdm(loader, desc=desc):\n",
    "        inputs = tokenizer(batch[\"prompt\"], return_tensors=\"pt\", padding=\"longest\").to(device)\n",
    "        with nethook.Trace(model, f\"transformer.h.{layer}\") as ret:\n",
    "            outputs = model(**inputs)\n",
    "            hiddens += ret.output[0]\n",
    "\n",
    "    results = []\n",
    "    for h, sample in zip(hiddens, samples):\n",
    "        subj_first_i, subj_first_j = sample[\"token_range_subject_first\"]\n",
    "        subj_last_i, subj_last_j = sample[\"token_range_subject_last\"]\n",
    "        result = {\n",
    "            \"h\": h,\n",
    "            \"h_attr_avg\": h[subj_first_j:subj_last_i - 1].mean(dim=0), # TODO: Upper bound not quite right\n",
    "            \"h_subj_avg_delt\": (h[subj_last_i:subj_last_j] - h[subj_first_i:subj_first_j]).mean(dim=0),\n",
    "            **sample,\n",
    "        }\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "preprocessed = precompute_hiddens(model, tokenizer, DATASET[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fafbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nethook\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "\n",
    "from src.utils import training\n",
    "\n",
    "train, val = training.random_split(preprocessed, hold_out=.1)\n",
    "train_loader = torch.utils.data.DataLoader(train)\n",
    "val_loader = torch.utils.data.DataLoader(val)\n",
    "probe = nn.Sequential(\n",
    "    nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(model.config.hidden_size, model.config.hidden_size),\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.AdamW(probe.parameters(), lr=LR)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "best, stopper = None, training.EarlyStopping(patience=PATIENCE)\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    if epoch != 0:\n",
    "        train_loss = 0.\n",
    "        probe.train()\n",
    "        for batch in train_loader:\n",
    "            inputs = batch[\"h_attr_avg\"]\n",
    "            targets = batch[\"h_subj_avg_delt\"]\n",
    "            predictions = probe(inputs)\n",
    "            loss = criterion(predictions, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "    else:\n",
    "        train_loss = float(\"inf\")\n",
    "\n",
    "    val_loss = 0.        \n",
    "    probe.eval()\n",
    "    for batch in val_loader:\n",
    "        inputs = batch[\"h_attr_avg\"]\n",
    "        targets = batch[\"h_subj_avg_delt\"]\n",
    "        predictions = probe(inputs)\n",
    "        loss = criterion(predictions, targets)\n",
    "        val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    print(f\"epoch {epoch} train={train_loss:.3f} val={val_loss:.3f}\")\n",
    "\n",
    "    if stopper(val_loss):\n",
    "        break\n",
    "    if stopper.improved:\n",
    "        best = probe.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b2e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def test_editor(model, tokenizer, editor, sample, layer=LAYER, alpha=1):\n",
    "    model.eval()\n",
    "    editor.eval()\n",
    "\n",
    "    def edit_output(output):\n",
    "        subj_first_i, subj_first_j = sample[\"token_range_subject_first\"]\n",
    "        subj_last_i, subj_last_j = sample[\"token_range_subject_last\"]\n",
    "        direction = editor(output[0][0:1, subj_first_j:subj_last_i - 1].mean(dim=1))\n",
    "        output[0][0:1, subj_last_i:subj_last_j] = output[0][0:1, subj_last_i:subj_last_j] + alpha * direction\n",
    "        return output\n",
    "    \n",
    "    inputs = tokenizer(sample[\"prompt\"], return_tensors=\"pt\").to(device)\n",
    "    with nethook.Trace(model, f\"transformer.h.{LAYER}\", edit_output=edit_output) as _:\n",
    "        outputs = model.generate(inputs.input_ids, max_new_tokens=7)\n",
    "\n",
    "    return tokenizer.batch_decode(outputs)\n",
    "\n",
    "test = DATASET[2]\n",
    "index = 10\n",
    "print(test[index])\n",
    "test_editor(model, tokenizer, probe, test[index], alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f48f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f9ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
