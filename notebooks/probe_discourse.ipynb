{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34b81b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "file = pathlib.Path('probing-discourse.json')\n",
    "with file.open('r') as handle:\n",
    "    samples = json.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f700524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'condition': {'name': 'real', 'occupation': 'real', 'context': 'primary'},\n",
       "  'labels': {'name': 'juan juarez fernandez', 'occupation': 'politician'},\n",
       "  'text': 'This is a story about juan juarez fernandez who works as a politician.'},\n",
       " {'condition': {'name': 'real', 'occupation': 'real', 'context': 'secondary'},\n",
       "  'labels': {'name': 'juan juarez fernandez', 'occupation': 'politician'},\n",
       "  'text': 'This is a story about juan juarez fernandez who forgot to bring a pseudonym to their job at the political party.'},\n",
       " {'condition': {'name': 'real', 'occupation': 'real', 'context': 'irrelevant'},\n",
       "  'labels': {'name': 'juan juarez fernandez', 'occupation': 'politician'},\n",
       "  'text': 'This is a story about juan juarez fernandez who climbed a hill.'},\n",
       " {'condition': {'name': 'real', 'occupation': 'fake', 'context': 'primary'},\n",
       "  'labels': {'name': 'juan juarez fernandez',\n",
       "   'occupation': 'table tennis player'},\n",
       "  'text': 'This is a story about juan juarez fernandez who works as a table tennis player.'},\n",
       " {'condition': {'name': 'real', 'occupation': 'fake', 'context': 'secondary'},\n",
       "  'labels': {'name': 'juan juarez fernandez',\n",
       "   'occupation': 'table tennis player'},\n",
       "  'text': 'This is a story about juan juarez fernandez who forgot to bring a table tennis racket to their job at the table tennis court.'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9aa04d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "device = 'cuda:1'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert = transformers.AutoModelForMaskedLM.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16a38797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "probe = torch.load('probe_wikidata_occupation.pth', map_location=device)\n",
    "indexer = torch.load('probe_wikidata_occupation_indexer.pth')\n",
    "unindexer = {index: label for label, index in indexer.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6937c568",
   "metadata": {},
   "source": [
    "# Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35567217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7e37f01e4b14de49ff6668a02e2d92f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "loader = data.DataLoader(samples, batch_size=64)\n",
    "\n",
    "predictions = []\n",
    "for batch in tqdm(loader):\n",
    "    with torch.inference_mode():\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding='longest').to(device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = bert(**inputs, return_dict=True, output_hidden_states=True)\n",
    "            reps = outputs.hidden_states[-1][:, 6]\n",
    "            topks = probe(reps).topk(k=5, dim=-1).indices.tolist()\n",
    "        predictions.extend([\n",
    "            [unindexer[idx] for idx in topk]\n",
    "            for topk in topks\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4560cdfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['politician',\n",
       "  'sportsperson',\n",
       "  'actor',\n",
       "  'association football player',\n",
       "  'artist'],\n",
       " ['sportsperson',\n",
       "  'handball player',\n",
       "  'badminton player',\n",
       "  'television presenter',\n",
       "  'artist'],\n",
       " ['samurai', 'sprinter', 'mangaka', 'sportsperson', 'musician'],\n",
       " ['sportsperson',\n",
       "  'volleyball player',\n",
       "  'handball player',\n",
       "  'badminton player',\n",
       "  'association football player'],\n",
       " ['sportsperson',\n",
       "  'field hockey player',\n",
       "  'handball player',\n",
       "  'volleyball player',\n",
       "  'table tennis player'],\n",
       " ['samurai', 'sprinter', 'mangaka', 'sportsperson', 'musician'],\n",
       " ['politician', 'businessperson', 'sportsperson', 'entrepreneur', 'mangaka'],\n",
       " ['sportsperson',\n",
       "  'comics artist',\n",
       "  'badminton player',\n",
       "  'businessperson',\n",
       "  'visual artist'],\n",
       " ['mangaka', 'samurai', 'musician', 'farmer', 'sprinter'],\n",
       " ['volleyball player',\n",
       "  'badminton player',\n",
       "  'sportsperson',\n",
       "  'basketball player',\n",
       "  'field hockey player']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9f2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "accuracies = collections.defaultdict(list)\n",
    "for sample, topk in zip(samples, predictions):\n",
    "    if sample['condition']['name'] != 'real' and sample['condition']['occupation'] == 'real':\n",
    "        continue\n",
    "    condition = frozenset(sample['condition'].items())\n",
    "    accuracies[condition].append(sample['labels']['occupation'] in topk)\n",
    "accuracies = {cond: sum(values) / len(values) for cond, values in accuracies.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9b6ffbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import tabulate\n",
    "\n",
    "table = [('context', 'name', 'occupation', 'accuracy')]\n",
    "for keys, accuracy in sorted(accuracies.items(), key=lambda kv: kv[-1], reverse=True):\n",
    "    keys = dict(keys)\n",
    "    row = [keys['context'], keys['name'], keys['occupation'], f'{accuracy:.3f}']\n",
    "    table.append(row)\n",
    "\n",
    "\n",
    "table_file = pathlib.Path('accuracy_table.txt')\n",
    "with table_file.open('w') as handle:\n",
    "    handle.write(tabulate.tabulate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a4888",
   "metadata": {},
   "source": [
    "# Connect with causal predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dd435ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "569ad790c79b483cbb54e94b8cabfd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "loader = data.DataLoader(samples, batch_size=64)\n",
    "\n",
    "probe_predictions, model_predictions = [], []\n",
    "for batch in tqdm(loader):\n",
    "    with torch.inference_mode():\n",
    "        inputs = tokenizer(batch['text'], return_tensors='pt', padding='longest').to(device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = bert(**inputs, return_dict=True, output_hidden_states=True)\n",
    "            reps = outputs.hidden_states[-1][:, 6]\n",
    "            probe_topks = probe(reps).topk(k=5, dim=-1).indices.tolist()\n",
    "        probe_predictions.extend([\n",
    "            [unindexer[idx] for idx in topk]\n",
    "            for topk in probe_topks\n",
    "        ])\n",
    "\n",
    "        texts = [\n",
    "            f'{text} Therefore, {name} works as a [MASK].'\n",
    "            for text, name in zip(batch['text'], batch['labels']['name'])\n",
    "        ]\n",
    "        inputs = tokenizer(texts, return_tensors='pt', padding='longest').to(device)\n",
    "        with torch.inference_mode():\n",
    "            outputs = bert(**inputs, return_dict=True, output_hidden_states=True)\n",
    "        batch_idx = torch.arange(len(texts))\n",
    "        token_idx = inputs.attention_mask.sum(dim=-1) - 3\n",
    "        model_predictions_ids = outputs.logits[batch_idx, token_idx].argmax(dim=-1)\n",
    "        model_predictions_tokens = tokenizer.batch_decode(model_predictions_ids.tolist())\n",
    "        model_predictions.extend(model_predictions_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ecc8ddf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------  ----  ----------  --------------  --------------  ---------\n",
      "context     name  occupation  probe accuracy  model accuracy  agreement\n",
      "primary     real  real        0.450           0.532           0.246\n",
      "secondary   real  real        0.232           0.206           0.103\n",
      "irrelevant  real  real        0.044           0.003           0.021\n",
      "primary     real  fake        0.358           0.421           0.181\n",
      "secondary   real  fake        0.170           0.123           0.074\n",
      "irrelevant  real  fake        0.034           0.003           0.021\n",
      "primary     fake  fake        0.512           0.290           0.142\n",
      "secondary   fake  fake        0.201           0.104           0.055\n",
      "irrelevant  fake  fake        0.035           0.004           0.000\n",
      "primary     none  fake        0.174           0.408           0.106\n",
      "secondary   none  fake        0.102           0.095           0.078\n",
      "irrelevant  none  fake        0.033           0.000           0.000\n",
      "----------  ----  ----------  --------------  --------------  ---------\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "probe_accuracies = collections.defaultdict(list)\n",
    "model_accuracies = collections.defaultdict(list)\n",
    "probe_model_agreements = collections.defaultdict(list)\n",
    "for sample, model_pred, probe_topk in zip(samples, model_predictions, probe_predictions):\n",
    "    if sample['condition']['name'] != 'real' and sample['condition']['occupation'] == 'real':\n",
    "        continue\n",
    "    condition = frozenset(sample['condition'].items())\n",
    "    probe_accuracies[condition].append(sample['labels']['occupation'] in probe_topk)\n",
    "    model_accuracies[condition].append(sample['labels']['occupation'] == model_pred)\n",
    "    probe_model_agreements[condition].append(model_pred in probe_topk)\n",
    "probe_accuracies = {cond: sum(values) / len(values) for cond, values in probe_accuracies.items()}\n",
    "model_accuracies = {cond: sum(values) / len(values) for cond, values in model_accuracies.items()}\n",
    "probe_model_agreements = {cond: sum(values) / len(values) for cond, values in probe_model_agreements.items()}\n",
    "\n",
    "table = [('context', 'name', 'occupation', 'probe accuracy', 'model accuracy', 'agreement')]\n",
    "for keys in sorted(probe_accuracies.keys()):\n",
    "    probe_accuracy = probe_accuracies[keys]\n",
    "    model_accuracy = model_accuracies[keys]\n",
    "    agreement = probe_model_agreements[keys]\n",
    "\n",
    "    keys = dict(keys)\n",
    "    row = [keys['context'], keys['name'], keys['occupation'], f'{probe_accuracy:.3f}', f'{model_accuracy:.3f}',\n",
    "           f'{agreement:.3f}']\n",
    "    table.append(row)\n",
    "\n",
    "\n",
    "table_file = pathlib.Path('accuracy_table.txt')\n",
    "with table_file.open('w') as handle:\n",
    "    handle.write(tabulate.tabulate(table))\n",
    "print(tabulate.tabulate(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b56dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
