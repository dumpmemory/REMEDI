{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a130973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ced43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_ROOT = Path(\"../results\")\n",
    "assert RESULTS_ROOT.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e289af",
   "metadata": {},
   "source": [
    "First, load our own results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3db244",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"icml_eval_fact_gen_gptj\"\n",
    "\n",
    "rows_by_method = {}\n",
    "for method in (r\"\\ourmethod\", \"prefix\", \"replace\"):\n",
    "    row = {\n",
    "        \"method\": method\n",
    "    }\n",
    "\n",
    "    if method == r\"\\ourmethod\":\n",
    "        results_dir = RESULTS_ROOT / EXPERIMENT_NAME / \"linear/1\"\n",
    "    else:\n",
    "        results_dir = RESULTS_ROOT / EXPERIMENT_NAME / method\n",
    "\n",
    "    for benchmark_name, keys in (\n",
    "#         (\"efficacy\", (\"score\", \"magnitude\")),\n",
    "        (\n",
    "            \"paraphrase\",\n",
    "            (\n",
    "                \"score\",\n",
    "#                 \"magnitude\",\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"generation\",\n",
    "            (\n",
    "#                 \"fluency\",\n",
    "                \"consistency\",\n",
    "            ),\n",
    "        ),\n",
    "        (\"essence\", (\"essence\",)),\n",
    "    ):\n",
    "        results_file = results_dir / f\"{benchmark_name}_metrics.json\"\n",
    "        print(f\"reading {results_file}\")\n",
    "        with results_file.open(\"r\") as handle:\n",
    "            results = json.load(handle)\n",
    "\n",
    "        for key in keys:\n",
    "            row[f\"{benchmark_name}_{key}\"] = results[key]\n",
    "\n",
    "    row[\"neighborhood_score\"] = {\"mean\": 1.0, \"std\": 0.0}\n",
    "\n",
    "    rows_by_method[method] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c8acd",
   "metadata": {},
   "source": [
    "Make table for representation-editing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_rows = []\n",
    "for method in (\"prefix\", \"replace\", r\"\\ourmethod\"):\n",
    "    row = rows_by_method[method]\n",
    "    formatted_row = [method.capitalize()]\n",
    "    for key in (\n",
    "#         \"efficacy_score\",\n",
    "#         \"efficacy_magnitude\",\n",
    "        \"paraphrase_score\",\n",
    "#         \"paraphrase_magnitude\",\n",
    "#         \"generation_fluency\",\n",
    "        \"neighborhood_score\",\n",
    "        \"generation_consistency\",\n",
    "        \"essence_essence\",\n",
    "    ):\n",
    "        metric = row[key]\n",
    "\n",
    "        mean = metric[\"mean\"] * 100\n",
    "        std = metric[\"std\"] * 100\n",
    "\n",
    "        interval = (1.96 * std) / 5000\n",
    "\n",
    "        formatted = f\"${mean:.1f}\" + r\" \\pm \" + f\"{interval:.2f}$\".lstrip(\"0\")\n",
    "        formatted_row.append(formatted)\n",
    "    formatted_rows.append(formatted_row)\n",
    "\n",
    "table = \"\"\n",
    "for formatted_row in formatted_rows:\n",
    "    if formatted_row == \"ROME\":\n",
    "        table += r\"\\midrule\" + \"\\n\"\n",
    "    table += \" & \".join(formatted_row) + r\" \\\\\" + \"\\n\"\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a8dbb",
   "metadata": {},
   "source": [
    "Make table for model-editing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f73cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from src import data, metrics\n",
    "\n",
    "# Load references from our own eval.\n",
    "essence_references_file = Path(\"../results/icml_eval_fact_gen_gptj/essence_references.txt\")\n",
    "with essence_references_file.open(\"r\") as handle:\n",
    "    references = handle.readlines()\n",
    "references = [[r.strip(\" \\n\")] for r in references if r.strip(\" \\n\")]\n",
    "print(len(references))\n",
    "\n",
    "# Load the counterfact vectorizer.\n",
    "tfidf_vectorizer = data.load_counterfact_tfidf_vectorizer()\n",
    "\n",
    "# Load their generations.\n",
    "for results_dir in (\n",
    "    \"../../rome/results/FT-essence/run_000\",\n",
    "    \"../../rome/results/ROME-essence/run_000\",\n",
    "):\n",
    "    case_files = sorted(Path(results_dir).glob(\"case*.json\"))\n",
    "\n",
    "    cases = []\n",
    "    for case_file in tqdm(case_files):\n",
    "        with case_file.open(\"r\") as handle:\n",
    "            case = json.load(handle)\n",
    "        cases.append(case)\n",
    "    \n",
    "    cases = sorted(cases, key=lambda case: case[\"case_id\"])[:5000]\n",
    "\n",
    "    generations = []\n",
    "    for case in cases:\n",
    "        generations.append([case[\"post\"][\"generation\"]])\n",
    "\n",
    "    score = metrics.average_tfidf_similarity(generations, references, tfidf_vectorizer)\n",
    "    print(results_dir, score)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b62b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "rows = []\n",
    "for results_dir in (\n",
    "    \"../../rome/results/FT/run_000\",\n",
    "    \"../../rome/results/ROME/run_000\",\n",
    "):\n",
    "    summary_file = Path(results_dir) / \"summary.json\"\n",
    "    with summary_file.open(\"r\") as handle:\n",
    "        summary = json.load(handle)\n",
    "\n",
    "    row = [str(results_dir).split(\"/\")[-2]]\n",
    "    for metric in (\n",
    "#         \"post_rewrite_success\",\n",
    "#         \"post_rewrite_diff\",\n",
    "        \"post_paraphrase_success\",\n",
    "#         \"post_paraphrase_diff\",\n",
    "#         \"post_ngram_entropy\",\n",
    "        \"post_neighborhood_success\",\n",
    "        \"post_reference_score\",\n",
    "    ):\n",
    "        mean, std = summary[metric]\n",
    "        interval = 1.96 * std / summary[\"num_cases\"]\n",
    "        row.append(\n",
    "            f\"{mean:.1f}\"\n",
    "            + r\" \\pm \"\n",
    "            + f\"{interval:.2f}\".lstrip(\"0\")\n",
    "        )\n",
    "    row.append(\"\")\n",
    "    rows.append(row)\n",
    "string = (r\" \\\\\" + \"\\n\").join([\n",
    "    \" & \".join(f\"${m}$\" if i > 0 and m else m for i, m in enumerate(row))\n",
    "    for row in rows\n",
    "]) + r\" \\\\\"\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691c642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
