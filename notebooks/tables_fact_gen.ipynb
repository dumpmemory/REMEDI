{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605ced43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "RESULTS_ROOT = Path(\"../results\")\n",
    "assert RESULTS_ROOT.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f89558",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_by_method = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e289af",
   "metadata": {},
   "source": [
    "First, load our own results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3db244",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"icml_eval_fact_gen_gptj\"\n",
    "\n",
    "for method in (r\"\\ourmethod\", \"prefix\", \"replace\"):\n",
    "    row = {\n",
    "        \"method\": method\n",
    "    }\n",
    "\n",
    "    if method == r\"\\ourmethod\":\n",
    "        results_dir = RESULTS_ROOT / EXPERIMENT_NAME / \"linear/1\"\n",
    "    else:\n",
    "        results_dir = RESULTS_ROOT / EXPERIMENT_NAME / method\n",
    "\n",
    "    for benchmark_name, keys in (\n",
    "        (\"efficacy\", (\"score\", \"magnitude\")),\n",
    "        (\"paraphrase\", (\"score\", \"magnitude\")),\n",
    "        (\"generation\", (\"fluency\", \"consistency\")),\n",
    "        (\"essence\", (\"essence\",)),\n",
    "    ):\n",
    "        results_file = results_dir / f\"{benchmark_name}_metrics.json\"\n",
    "        print(f\"reading {results_file}\")\n",
    "        with results_file.open(\"r\") as handle:\n",
    "            results = json.load(handle)\n",
    "\n",
    "        for key in keys:\n",
    "            row[f\"{benchmark_name}_{key}\"] = results[key]\n",
    "\n",
    "    rows_by_method[method] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610feb0",
   "metadata": {},
   "source": [
    "Then, load results from ROME code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b62b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "rows = []\n",
    "for results_dir in (\n",
    "    \"../../rome/results/FT/run_000\",\n",
    "    \"../../rome/results/ROME/run_000\",\n",
    "):\n",
    "    summary_file = Path(results_dir) / \"summary.json\"\n",
    "    with summary_file.open(\"r\") as handle:\n",
    "        summary = json.load(handle)\n",
    "    \n",
    "    row = [str(results_dir).split(\"/\")[-2]]\n",
    "    for metric in (\n",
    "        \"post_rewrite_success\",\n",
    "        \"post_rewrite_diff\",\n",
    "        \"post_paraphrase_success\",\n",
    "        \"post_paraphrase_diff\",\n",
    "        \"post_ngram_entropy\",\n",
    "        \"post_reference_score\",\n",
    "    ):\n",
    "        mean, std = summary[metric]\n",
    "        interval = 1.96 * std / summary[\"num_cases\"]\n",
    "        row.append(\n",
    "            f\"{mean:.1f}\"\n",
    "            + r\" \\pm \"\n",
    "            + f\"{interval:.2f}\".lstrip(\"0\")\n",
    "        )\n",
    "    row.append(\"\")\n",
    "    rows.append(row)\n",
    "string = (r\" \\\\\" + \"\\n\").join([\n",
    "    \" & \".join(f\"${m}$\" if i > 0 and m else m for i, m in enumerate(row))\n",
    "    for row in rows\n",
    "]) + r\" \\\\\"\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c8acd",
   "metadata": {},
   "source": [
    "Now make the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_rows = []\n",
    "for method in (\"prefix\", \"replace\", r\"\\ourmethod\"):\n",
    "    row = rows_by_method[method]\n",
    "    formatted_row = [method.capitalize()]\n",
    "    for key in (\n",
    "        \"efficacy_score\",\n",
    "        \"efficacy_magnitude\",\n",
    "        \"paraphrase_score\",\n",
    "        \"paraphrase_magnitude\",\n",
    "        \"generation_fluency\",\n",
    "        \"generation_consistency\",\n",
    "        \"essence_essence\",\n",
    "    ):\n",
    "        metric = row[key]\n",
    "\n",
    "        mean = metric[\"mean\"] * 100\n",
    "        std = metric[\"std\"] * 100\n",
    "\n",
    "        interval = (1.96 * std) / 5000\n",
    "\n",
    "        formatted = f\"${mean:.1f}\" + r\" \\pm \" + f\"{interval:.2f}$\".lstrip(\"0\")\n",
    "        formatted_row.append(formatted)\n",
    "    formatted_rows.append(formatted_row)\n",
    "\n",
    "table = \"\"\n",
    "for formatted_row in formatted_rows:\n",
    "    if formatted_row == \"ROME\":\n",
    "        table += r\"\\midrule\" + \"\\n\"\n",
    "    table += \" & \".join(formatted_row) + r\" \\\\\" + \"\\n\"\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691c642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
