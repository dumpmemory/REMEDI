"""Logic for getting and mucking with model hidden representations."""
from typing import Optional, Sequence

from src.utils import model_utils
from src.utils.typing import Dataset, Device, StrSequence

import torch
from baukit import nethook

PrecomputedHiddens = dict[int, torch.Tensor]


@torch.inference_mode()
def hiddens_for_batch(
    mt: model_utils.ModelAndTokenizer,
    batch: StrSequence,
    layers: Optional[Sequence[int]] = None,
    layer_paths: Optional[StrSequence] = None,
    device: Optional[Device] = None,
) -> PrecomputedHiddens:
    """Compute hidden representations for the batch."""
    if layers is not None and layer_paths is not None:
        raise ValueError("cannot set both `layers` and `layer_paths`")
    if layers is None:
        layers = model_utils.determine_layers(mt)
    if layer_paths is None:
        layer_paths = model_utils.determine_layer_paths(mt, layers=layers)
    mt.model.to(device)
    with nethook.TraceDict(mt.model, layers=layer_paths) as ret:
        inputs = mt.tokenizer(
            batch, padding="longest", truncation=True, return_tensors="pt"
        ).to(device)
        mt.model(**inputs)
        hiddens = {
            layer: ret.output[layer_path][0]
            for layer, layer_path in zip(layers, layer_paths)
        }
    return hiddens


def hiddens_for_dataset(
    mt: model_utils.ModelAndTokenizer,
    dataset: Dataset,
    columns: StrSequence = ("context",),
    layers: Optional[Sequence[int]] = None,
    device: Optional[Device] = None,
    batch_size: int = 1,
    desc: Optional[str] = None,
) -> Dataset:
    """Precompute hidden representations for all samples in the dataset.

    Args:
        mt: The model and tokenizer.
        dataset: Dataset to compute hiddens for.
        columns: Columns to compute hiddens for. Defaults to just "context".
        layers: Layer numbers to compute hiddens for. Defaults to all.
        device: Send model and inputs to this device. Defaults to cpu.
        batch_size: Number of inputs to feed model at once. Defaults to 1.
            Note the return dataset will maintain all necessary padding tokens, meaning
            if you plan to batch if again, you will have to use the same batch size
            and not shuffle it for PyTorch data loaders to work properly.
        desc: Description to show on progress bar. Generated by default.

    Returns:
        Original dataset, but with additional fields of the form
            `{"hiddens": {"column_name": {0: ..., 1: ...}}}` where the int keys are layer numbers.

    """
    if layers is None:
        layers = model_utils.determine_layers(mt)
    layer_paths = model_utils.determine_layer_paths(mt, layers=layers)

    if desc is None:
        layers_listed = ", ".join(str(l) for l in layers)
        columns_listed = ", ".join(columns)
        desc = f"compute l=[{layers_listed}] c=[{columns_listed}]"

    return dataset.map(
        lambda batch: {
            "hiddens": {
                column: {
                    hiddens_for_batch(
                        mt, batch[column], layer_paths=layer_paths, device=device
                    )
                }
                for column in columns
            }
        },
        batch_size=batch_size,
        desc=desc,
    )
